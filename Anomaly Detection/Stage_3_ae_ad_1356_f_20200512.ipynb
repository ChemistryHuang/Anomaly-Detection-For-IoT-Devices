{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Autoencoder for Anomaly Detection for IoT 1,3,5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder (AE) anomaly detection model used in this project work as follow:\n",
    "\n",
    "1. we only used the benign data when training the AE model\n",
    "2. It identify anomalies by trying to recreate the input data\n",
    "3. When the model fail to create the input with low RMSE, we can conclude that it's an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all random variables\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import all required library\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers, Model\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all afunctions\n",
    "\n",
    "#Create predict function\n",
    "def predict(mse, treshold):\n",
    "    prd = []\n",
    "    for i in mse:\n",
    "        if i > treshold:\n",
    "            prd.append(1)\n",
    "        else: prd.append(0)\n",
    "            \n",
    "    return np.asarray(prd)\n",
    "\n",
    "#Create RMSE function that acccept numpy array\n",
    "def np_rmse(y_pred, y_true):\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2), axis=1))\n",
    "\n",
    "#Create RMSE function for Keras model\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the AE model on IoT 1,3,5 and 6 benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the benign data\n",
    "device3 = pd.read_csv(\"3.benign.csv\")\n",
    "device6 = pd.read_csv(\"6.benign.csv\")\n",
    "device1 = pd.read_csv(\"1.benign.csv\")\n",
    "device5 = pd.read_csv(\"5.benign.csv\")\n",
    "\n",
    "all_benign = pd.concat([device6,device3,device1, device5])\n",
    "all_benign['label'] = 0\n",
    "\n",
    "#split to train and test\n",
    "all_benign, all_benign_test,_,_ = train_test_split(all_benign.iloc[:,:-1].values, \n",
    "                                all_benign.iloc[:,-1].values, test_size=0.5, random_state=42)\n",
    "\n",
    "#scale train benign data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_benign)\n",
    "all_benign = scaler.transform(all_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99726 samples, validate on 24932 samples\n",
      "Epoch 1/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.2822 - val_loss: 0.2202\n",
      "Epoch 2/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.1649 - val_loss: 0.1829\n",
      "Epoch 3/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.1351 - val_loss: 0.1293\n",
      "Epoch 4/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.1203 - val_loss: 0.1540\n",
      "Epoch 5/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.1114 - val_loss: 0.1200\n",
      "Epoch 6/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.1063 - val_loss: 0.1019\n",
      "Epoch 7/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.1007 - val_loss: 0.1129\n",
      "Epoch 8/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0976 - val_loss: 0.1233\n",
      "Epoch 9/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0944 - val_loss: 0.0952\n",
      "Epoch 10/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0910 - val_loss: 0.1190\n",
      "Epoch 11/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0885 - val_loss: 0.1051\n",
      "Epoch 12/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0867 - val_loss: 0.1048\n",
      "Epoch 13/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0846 - val_loss: 0.0924\n",
      "Epoch 14/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0826 - val_loss: 0.0850\n",
      "Epoch 15/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0808 - val_loss: 0.0975\n",
      "Epoch 16/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0789 - val_loss: 0.0869\n",
      "Epoch 17/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0779 - val_loss: 0.0933\n",
      "Epoch 18/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0770 - val_loss: 0.0841\n",
      "Epoch 19/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0753 - val_loss: 0.0809\n",
      "Epoch 20/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0743 - val_loss: 0.0751\n",
      "Epoch 21/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0730 - val_loss: 0.0738\n",
      "Epoch 22/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0719 - val_loss: 0.0808\n",
      "Epoch 23/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0712 - val_loss: 0.0779\n",
      "Epoch 24/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0701 - val_loss: 0.0717\n",
      "Epoch 25/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0694 - val_loss: 0.0802\n",
      "Epoch 26/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0687 - val_loss: 0.0733\n",
      "Epoch 27/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0681 - val_loss: 0.0726\n",
      "Epoch 28/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0675 - val_loss: 0.0807\n",
      "Epoch 29/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0670 - val_loss: 0.0815\n",
      "Epoch 30/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0662 - val_loss: 0.0749\n",
      "Epoch 31/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0660 - val_loss: 0.0687\n",
      "Epoch 32/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0652 - val_loss: 0.0721\n",
      "Epoch 33/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0649 - val_loss: 0.0734\n",
      "Epoch 34/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0645 - val_loss: 0.0835\n",
      "Epoch 35/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0639 - val_loss: 0.0807\n",
      "Epoch 36/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0635 - val_loss: 0.0647\n",
      "Epoch 37/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0630 - val_loss: 0.0632\n",
      "Epoch 38/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0627 - val_loss: 0.0663\n",
      "Epoch 39/450\n",
      "99726/99726 [==============================] - 3s 32us/step - loss: 0.0625 - val_loss: 0.0720\n",
      "Epoch 40/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0620 - val_loss: 0.0708\n",
      "Epoch 41/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0617 - val_loss: 0.0671\n",
      "Epoch 42/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0612 - val_loss: 0.0714\n",
      "Epoch 43/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0608 - val_loss: 0.0669\n",
      "Epoch 44/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0609 - val_loss: 0.0667\n",
      "Epoch 45/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0605 - val_loss: 0.0684\n",
      "Epoch 46/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0603 - val_loss: 0.0611\n",
      "Epoch 47/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0599 - val_loss: 0.0668\n",
      "Epoch 48/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0598 - val_loss: 0.0644\n",
      "Epoch 49/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0593 - val_loss: 0.0659\n",
      "Epoch 50/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0594 - val_loss: 0.0639\n",
      "Epoch 51/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0590 - val_loss: 0.0702\n",
      "Epoch 52/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0587 - val_loss: 0.0728\n",
      "Epoch 53/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0587 - val_loss: 0.0672\n",
      "Epoch 54/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0584 - val_loss: 0.0614\n",
      "Epoch 55/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0582 - val_loss: 0.0634\n",
      "Epoch 56/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0579 - val_loss: 0.0707\n",
      "Epoch 57/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0577 - val_loss: 0.0736\n",
      "Epoch 58/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0576 - val_loss: 0.0665\n",
      "Epoch 59/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0574 - val_loss: 0.0700\n",
      "Epoch 60/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0573 - val_loss: 0.0676\n",
      "Epoch 61/450\n",
      "99726/99726 [==============================] - 3s 32us/step - loss: 0.0569 - val_loss: 0.0618\n",
      "Epoch 62/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0568 - val_loss: 0.0723\n",
      "Epoch 63/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0567 - val_loss: 0.0678\n",
      "Epoch 64/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0565 - val_loss: 0.0598\n",
      "Epoch 65/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0565 - val_loss: 0.0596\n",
      "Epoch 66/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0562 - val_loss: 0.0630\n",
      "Epoch 67/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0562 - val_loss: 0.0600\n",
      "Epoch 68/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0559 - val_loss: 0.0645\n",
      "Epoch 69/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0559 - val_loss: 0.0659\n",
      "Epoch 70/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0556 - val_loss: 0.0604\n",
      "Epoch 71/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0555 - val_loss: 0.0638\n",
      "Epoch 72/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0555 - val_loss: 0.0635\n",
      "Epoch 73/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0552 - val_loss: 0.0645\n",
      "Epoch 74/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0551 - val_loss: 0.0590\n",
      "Epoch 75/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0550 - val_loss: 0.0582\n",
      "Epoch 76/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0548 - val_loss: 0.0592\n",
      "Epoch 77/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0546 - val_loss: 0.0606\n",
      "Epoch 78/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0547 - val_loss: 0.0593\n",
      "Epoch 79/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0545 - val_loss: 0.0668\n",
      "Epoch 80/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0543 - val_loss: 0.0692\n",
      "Epoch 81/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0542 - val_loss: 0.0596\n",
      "Epoch 82/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0541 - val_loss: 0.0573\n",
      "Epoch 83/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0539 - val_loss: 0.0568\n",
      "Epoch 84/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0540 - val_loss: 0.0611\n",
      "Epoch 85/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0538 - val_loss: 0.0587\n",
      "Epoch 86/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0537 - val_loss: 0.0592\n",
      "Epoch 87/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0535 - val_loss: 0.0640\n",
      "Epoch 88/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0535 - val_loss: 0.0585\n",
      "Epoch 89/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0533 - val_loss: 0.0614\n",
      "Epoch 90/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0532 - val_loss: 0.0601\n",
      "Epoch 91/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0531 - val_loss: 0.0635\n",
      "Epoch 92/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0529 - val_loss: 0.0588\n",
      "Epoch 93/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0531 - val_loss: 0.0558\n",
      "Epoch 94/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0528 - val_loss: 0.0588\n",
      "Epoch 95/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0528 - val_loss: 0.0679\n",
      "Epoch 96/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0526 - val_loss: 0.0574\n",
      "Epoch 97/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0524 - val_loss: 0.0536\n",
      "Epoch 98/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0524 - val_loss: 0.0632\n",
      "Epoch 99/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0524 - val_loss: 0.0578\n",
      "Epoch 100/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0525 - val_loss: 0.0599\n",
      "Epoch 101/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0522 - val_loss: 0.0542\n",
      "Epoch 102/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0520 - val_loss: 0.0580\n",
      "Epoch 103/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0520 - val_loss: 0.0548\n",
      "Epoch 104/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0519 - val_loss: 0.0626\n",
      "Epoch 105/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0518 - val_loss: 0.0601\n",
      "Epoch 106/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0519 - val_loss: 0.0677\n",
      "Epoch 107/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0519 - val_loss: 0.0583\n",
      "Epoch 108/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0516 - val_loss: 0.0581\n",
      "Epoch 109/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0516 - val_loss: 0.0608\n",
      "Epoch 110/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0515 - val_loss: 0.0573\n",
      "Epoch 111/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0513 - val_loss: 0.0607\n",
      "Epoch 112/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0514 - val_loss: 0.0632\n",
      "Epoch 113/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0512 - val_loss: 0.0657\n",
      "Epoch 114/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0510 - val_loss: 0.0564\n",
      "Epoch 115/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0510 - val_loss: 0.0595\n",
      "Epoch 116/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0510 - val_loss: 0.0549\n",
      "Epoch 117/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0508 - val_loss: 0.0708\n",
      "Epoch 118/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0509 - val_loss: 0.0604\n",
      "Epoch 119/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0509 - val_loss: 0.0534\n",
      "Epoch 120/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0508 - val_loss: 0.0567\n",
      "Epoch 121/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0506 - val_loss: 0.0552\n",
      "Epoch 122/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0505 - val_loss: 0.0575\n",
      "Epoch 123/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0505 - val_loss: 0.0567\n",
      "Epoch 124/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0504 - val_loss: 0.0531\n",
      "Epoch 125/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0502 - val_loss: 0.0611\n",
      "Epoch 126/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0504 - val_loss: 0.0588\n",
      "Epoch 127/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0503 - val_loss: 0.0603\n",
      "Epoch 128/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0502 - val_loss: 0.0578\n",
      "Epoch 129/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0501 - val_loss: 0.0576\n",
      "Epoch 130/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0500 - val_loss: 0.0612\n",
      "Epoch 131/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0500 - val_loss: 0.0569\n",
      "Epoch 132/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0498 - val_loss: 0.0544\n",
      "Epoch 133/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0500 - val_loss: 0.0507\n",
      "Epoch 134/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0498 - val_loss: 0.0575\n",
      "Epoch 135/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0497 - val_loss: 0.0511\n",
      "Epoch 136/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0498 - val_loss: 0.0619\n",
      "Epoch 137/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0498 - val_loss: 0.0561\n",
      "Epoch 138/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0497 - val_loss: 0.0598\n",
      "Epoch 139/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0497 - val_loss: 0.0586\n",
      "Epoch 140/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0496 - val_loss: 0.0528\n",
      "Epoch 141/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0495 - val_loss: 0.0648\n",
      "Epoch 142/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0493 - val_loss: 0.0535\n",
      "Epoch 143/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0492 - val_loss: 0.0520\n",
      "Epoch 144/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0493 - val_loss: 0.0602\n",
      "Epoch 145/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0491 - val_loss: 0.0497\n",
      "Epoch 146/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0493 - val_loss: 0.0577\n",
      "Epoch 147/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0493 - val_loss: 0.0516\n",
      "Epoch 148/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0491 - val_loss: 0.0529\n",
      "Epoch 149/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0489 - val_loss: 0.0594\n",
      "Epoch 150/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0490 - val_loss: 0.0535\n",
      "Epoch 151/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0491 - val_loss: 0.0595\n",
      "Epoch 152/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0488 - val_loss: 0.0545\n",
      "Epoch 153/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0489 - val_loss: 0.0556\n",
      "Epoch 154/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0488 - val_loss: 0.0553\n",
      "Epoch 155/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0486 - val_loss: 0.0566\n",
      "Epoch 156/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0487 - val_loss: 0.0539\n",
      "Epoch 157/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0488 - val_loss: 0.0500\n",
      "Epoch 158/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0487 - val_loss: 0.0527\n",
      "Epoch 159/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0486 - val_loss: 0.0562\n",
      "Epoch 160/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0485 - val_loss: 0.0538\n",
      "Epoch 161/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0484 - val_loss: 0.0545\n",
      "Epoch 162/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0483 - val_loss: 0.0537\n",
      "Epoch 163/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0484 - val_loss: 0.0647\n",
      "Epoch 164/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0483 - val_loss: 0.0558\n",
      "Epoch 165/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0483 - val_loss: 0.0545\n",
      "Epoch 166/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0483 - val_loss: 0.0509\n",
      "Epoch 167/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0482 - val_loss: 0.0593\n",
      "Epoch 168/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0482 - val_loss: 0.0516\n",
      "Epoch 169/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0483 - val_loss: 0.0538\n",
      "Epoch 170/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0480 - val_loss: 0.0547\n",
      "Epoch 171/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0481 - val_loss: 0.0584\n",
      "Epoch 172/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0480 - val_loss: 0.0525\n",
      "Epoch 173/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0479 - val_loss: 0.0528\n",
      "Epoch 174/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0478 - val_loss: 0.0567\n",
      "Epoch 175/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0480 - val_loss: 0.0526\n",
      "Epoch 176/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0478 - val_loss: 0.0504\n",
      "Epoch 177/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0479 - val_loss: 0.0510\n",
      "Epoch 178/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0477 - val_loss: 0.0542\n",
      "Epoch 179/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0477 - val_loss: 0.0508\n",
      "Epoch 180/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0477 - val_loss: 0.0520\n",
      "Epoch 181/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0477 - val_loss: 0.0553\n",
      "Epoch 182/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0477 - val_loss: 0.0563\n",
      "Epoch 183/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0475 - val_loss: 0.0514\n",
      "Epoch 184/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0476 - val_loss: 0.0543\n",
      "Epoch 185/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0474 - val_loss: 0.0554\n",
      "Epoch 186/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0473 - val_loss: 0.0525\n",
      "Epoch 187/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0474 - val_loss: 0.0538\n",
      "Epoch 188/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0476 - val_loss: 0.0506\n",
      "Epoch 189/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0474 - val_loss: 0.0517\n",
      "Epoch 190/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0472 - val_loss: 0.0564\n",
      "Epoch 191/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0472 - val_loss: 0.0465\n",
      "Epoch 192/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0472 - val_loss: 0.0585\n",
      "Epoch 193/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0471 - val_loss: 0.0610\n",
      "Epoch 194/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0472 - val_loss: 0.0505\n",
      "Epoch 195/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0471 - val_loss: 0.0576\n",
      "Epoch 196/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0472 - val_loss: 0.0540\n",
      "Epoch 197/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0472 - val_loss: 0.0551\n",
      "Epoch 198/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0470 - val_loss: 0.0581\n",
      "Epoch 199/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0470 - val_loss: 0.0514\n",
      "Epoch 200/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0469 - val_loss: 0.0557\n",
      "Epoch 201/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0468 - val_loss: 0.0515\n",
      "Epoch 202/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0469 - val_loss: 0.0526\n",
      "Epoch 203/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0469 - val_loss: 0.0608\n",
      "Epoch 204/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0468 - val_loss: 0.0508\n",
      "Epoch 205/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0468 - val_loss: 0.0534\n",
      "Epoch 206/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0467 - val_loss: 0.0556\n",
      "Epoch 207/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0467 - val_loss: 0.0524\n",
      "Epoch 208/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0467 - val_loss: 0.0518\n",
      "Epoch 209/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0466 - val_loss: 0.0537\n",
      "Epoch 210/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0465 - val_loss: 0.0661\n",
      "Epoch 211/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0465 - val_loss: 0.0581\n",
      "Epoch 212/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0465 - val_loss: 0.0585\n",
      "Epoch 213/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0465 - val_loss: 0.0487\n",
      "Epoch 214/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0465 - val_loss: 0.0616\n",
      "Epoch 215/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0464 - val_loss: 0.0477\n",
      "Epoch 216/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0465 - val_loss: 0.0457\n",
      "Epoch 217/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0464 - val_loss: 0.0549\n",
      "Epoch 218/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0464 - val_loss: 0.0480\n",
      "Epoch 219/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0464 - val_loss: 0.0531\n",
      "Epoch 220/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0463 - val_loss: 0.0540\n",
      "Epoch 221/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0462 - val_loss: 0.0518\n",
      "Epoch 222/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0463 - val_loss: 0.0647\n",
      "Epoch 223/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0463 - val_loss: 0.0539\n",
      "Epoch 224/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0462 - val_loss: 0.0584\n",
      "Epoch 225/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0461 - val_loss: 0.0562\n",
      "Epoch 226/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0462 - val_loss: 0.0505\n",
      "Epoch 227/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0460 - val_loss: 0.0491\n",
      "Epoch 228/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0460 - val_loss: 0.0526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0459 - val_loss: 0.0567\n",
      "Epoch 230/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0460 - val_loss: 0.0586\n",
      "Epoch 231/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0460 - val_loss: 0.0536\n",
      "Epoch 232/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0460 - val_loss: 0.0476\n",
      "Epoch 233/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0460 - val_loss: 0.0518\n",
      "Epoch 234/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0460 - val_loss: 0.0506\n",
      "Epoch 235/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0458 - val_loss: 0.0495\n",
      "Epoch 236/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0458 - val_loss: 0.0528\n",
      "Epoch 237/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0457 - val_loss: 0.0550\n",
      "Epoch 238/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0457 - val_loss: 0.0523\n",
      "Epoch 239/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0457 - val_loss: 0.0497\n",
      "Epoch 240/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0457 - val_loss: 0.0524\n",
      "Epoch 241/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0456 - val_loss: 0.0539\n",
      "Epoch 242/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0457 - val_loss: 0.0522\n",
      "Epoch 243/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0455 - val_loss: 0.0485\n",
      "Epoch 244/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0456 - val_loss: 0.0551\n",
      "Epoch 245/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0455 - val_loss: 0.0594\n",
      "Epoch 246/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0456 - val_loss: 0.0519\n",
      "Epoch 247/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0454 - val_loss: 0.0550\n",
      "Epoch 248/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0454 - val_loss: 0.0478\n",
      "Epoch 249/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0455 - val_loss: 0.0497\n",
      "Epoch 250/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0453 - val_loss: 0.0511\n",
      "Epoch 251/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0453 - val_loss: 0.0501\n",
      "Epoch 252/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0453 - val_loss: 0.0525\n",
      "Epoch 253/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0453 - val_loss: 0.0541\n",
      "Epoch 254/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0452 - val_loss: 0.0563\n",
      "Epoch 255/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0452 - val_loss: 0.0476\n",
      "Epoch 256/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0452 - val_loss: 0.0504\n",
      "Epoch 257/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0452 - val_loss: 0.0561\n",
      "Epoch 258/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0452 - val_loss: 0.0496\n",
      "Epoch 259/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0451 - val_loss: 0.0503\n",
      "Epoch 260/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0452 - val_loss: 0.0531\n",
      "Epoch 261/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0451 - val_loss: 0.0546\n",
      "Epoch 262/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0451 - val_loss: 0.0516\n",
      "Epoch 263/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0451 - val_loss: 0.0556\n",
      "Epoch 264/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0449 - val_loss: 0.0527\n",
      "Epoch 265/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0451 - val_loss: 0.0472\n",
      "Epoch 266/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0449 - val_loss: 0.0506\n",
      "Epoch 267/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0449 - val_loss: 0.0520\n",
      "Epoch 268/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0448 - val_loss: 0.0515\n",
      "Epoch 269/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0449 - val_loss: 0.0469\n",
      "Epoch 270/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0449 - val_loss: 0.0475\n",
      "Epoch 271/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0447 - val_loss: 0.0473\n",
      "Epoch 272/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0447 - val_loss: 0.0494\n",
      "Epoch 273/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0446 - val_loss: 0.0508\n",
      "Epoch 274/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0447 - val_loss: 0.0506\n",
      "Epoch 275/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0447 - val_loss: 0.0500\n",
      "Epoch 276/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0447 - val_loss: 0.0562\n",
      "Epoch 277/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0446 - val_loss: 0.0533\n",
      "Epoch 278/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0447 - val_loss: 0.0507\n",
      "Epoch 279/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0445 - val_loss: 0.0514\n",
      "Epoch 280/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0446 - val_loss: 0.0552\n",
      "Epoch 281/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0446 - val_loss: 0.0504\n",
      "Epoch 282/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0445 - val_loss: 0.0517\n",
      "Epoch 283/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0446 - val_loss: 0.0492\n",
      "Epoch 284/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0444 - val_loss: 0.0445\n",
      "Epoch 285/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0445 - val_loss: 0.0476\n",
      "Epoch 286/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0443 - val_loss: 0.0536\n",
      "Epoch 287/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0443 - val_loss: 0.0479\n",
      "Epoch 288/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0443 - val_loss: 0.0481\n",
      "Epoch 289/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0444 - val_loss: 0.0540\n",
      "Epoch 290/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0443 - val_loss: 0.0505\n",
      "Epoch 291/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0443 - val_loss: 0.0466\n",
      "Epoch 292/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0442 - val_loss: 0.0488\n",
      "Epoch 293/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0442 - val_loss: 0.0586\n",
      "Epoch 294/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0442 - val_loss: 0.0517\n",
      "Epoch 295/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0442 - val_loss: 0.0459\n",
      "Epoch 296/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0441 - val_loss: 0.0502\n",
      "Epoch 297/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0441 - val_loss: 0.0527\n",
      "Epoch 298/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0441 - val_loss: 0.0477\n",
      "Epoch 299/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0441 - val_loss: 0.0525\n",
      "Epoch 300/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0440 - val_loss: 0.0502\n",
      "Epoch 301/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0440 - val_loss: 0.0504\n",
      "Epoch 302/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0439 - val_loss: 0.0529\n",
      "Epoch 303/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0440 - val_loss: 0.0463\n",
      "Epoch 304/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0439 - val_loss: 0.0506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0440 - val_loss: 0.0560\n",
      "Epoch 306/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0440 - val_loss: 0.0486\n",
      "Epoch 307/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0438 - val_loss: 0.0472\n",
      "Epoch 308/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0437 - val_loss: 0.0552\n",
      "Epoch 309/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0438 - val_loss: 0.0504\n",
      "Epoch 310/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0438 - val_loss: 0.0557\n",
      "Epoch 311/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0437 - val_loss: 0.0470\n",
      "Epoch 312/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0437 - val_loss: 0.0554\n",
      "Epoch 313/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0437 - val_loss: 0.0463\n",
      "Epoch 314/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0437 - val_loss: 0.0488\n",
      "Epoch 315/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0438 - val_loss: 0.0469\n",
      "Epoch 316/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0436 - val_loss: 0.0483\n",
      "Epoch 317/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0436 - val_loss: 0.0491\n",
      "Epoch 318/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0436 - val_loss: 0.0490\n",
      "Epoch 319/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0436 - val_loss: 0.0497\n",
      "Epoch 320/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0437 - val_loss: 0.0494\n",
      "Epoch 321/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0436 - val_loss: 0.0449\n",
      "Epoch 322/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0435 - val_loss: 0.0532\n",
      "Epoch 323/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0436 - val_loss: 0.0541\n",
      "Epoch 324/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0435 - val_loss: 0.0478\n",
      "Epoch 325/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0435 - val_loss: 0.0520\n",
      "Epoch 326/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0434 - val_loss: 0.0469\n",
      "Epoch 327/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0434 - val_loss: 0.0483\n",
      "Epoch 328/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0434 - val_loss: 0.0491\n",
      "Epoch 329/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0433 - val_loss: 0.0501\n",
      "Epoch 330/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0434 - val_loss: 0.0454\n",
      "Epoch 331/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0433 - val_loss: 0.0545\n",
      "Epoch 332/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0433 - val_loss: 0.0483\n",
      "Epoch 333/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0433 - val_loss: 0.0474\n",
      "Epoch 334/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0432 - val_loss: 0.0514\n",
      "Epoch 335/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0433 - val_loss: 0.0485\n",
      "Epoch 336/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0432 - val_loss: 0.0547\n",
      "Epoch 337/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0548\n",
      "Epoch 338/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0432 - val_loss: 0.0454\n",
      "Epoch 339/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0432 - val_loss: 0.0542\n",
      "Epoch 340/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0432 - val_loss: 0.0539\n",
      "Epoch 341/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0432 - val_loss: 0.0446\n",
      "Epoch 342/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0541\n",
      "Epoch 343/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0431 - val_loss: 0.0549\n",
      "Epoch 344/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0479\n",
      "Epoch 345/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0466\n",
      "Epoch 346/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0430 - val_loss: 0.0477\n",
      "Epoch 347/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0430 - val_loss: 0.0465\n",
      "Epoch 348/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0430 - val_loss: 0.0498\n",
      "Epoch 349/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0503\n",
      "Epoch 350/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0430 - val_loss: 0.0526\n",
      "Epoch 351/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0431 - val_loss: 0.0489\n",
      "Epoch 352/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0430 - val_loss: 0.0460\n",
      "Epoch 353/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0429 - val_loss: 0.0511\n",
      "Epoch 354/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0429 - val_loss: 0.0526\n",
      "Epoch 355/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0428 - val_loss: 0.0549\n",
      "Epoch 356/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0429 - val_loss: 0.0513\n",
      "Epoch 357/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0428 - val_loss: 0.0470\n",
      "Epoch 358/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0429 - val_loss: 0.0487\n",
      "Epoch 359/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0429 - val_loss: 0.0498\n",
      "Epoch 360/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0429 - val_loss: 0.0534\n",
      "Epoch 361/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0427 - val_loss: 0.0490\n",
      "Epoch 362/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0427 - val_loss: 0.0454\n",
      "Epoch 363/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0427 - val_loss: 0.0512\n",
      "Epoch 364/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0428 - val_loss: 0.0493\n",
      "Epoch 365/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0428 - val_loss: 0.0486\n",
      "Epoch 366/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0428 - val_loss: 0.0466\n",
      "Epoch 367/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0427 - val_loss: 0.0490\n",
      "Epoch 368/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0427 - val_loss: 0.0498\n",
      "Epoch 369/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0492\n",
      "Epoch 370/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0453\n",
      "Epoch 371/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0447\n",
      "Epoch 372/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0528\n",
      "Epoch 373/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0483\n",
      "Epoch 374/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0425 - val_loss: 0.0512\n",
      "Epoch 375/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0477\n",
      "Epoch 376/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0427 - val_loss: 0.0443\n",
      "Epoch 377/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0425 - val_loss: 0.0492\n",
      "Epoch 378/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0425 - val_loss: 0.0514\n",
      "Epoch 379/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0425 - val_loss: 0.0536\n",
      "Epoch 380/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0425 - val_loss: 0.0442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0425 - val_loss: 0.0498\n",
      "Epoch 382/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0425 - val_loss: 0.0447\n",
      "Epoch 383/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0426 - val_loss: 0.0520\n",
      "Epoch 384/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0424 - val_loss: 0.0470\n",
      "Epoch 385/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0423 - val_loss: 0.0605\n",
      "Epoch 386/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0424 - val_loss: 0.0484\n",
      "Epoch 387/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0423 - val_loss: 0.0467\n",
      "Epoch 388/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0423 - val_loss: 0.0445\n",
      "Epoch 389/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0423 - val_loss: 0.0543\n",
      "Epoch 390/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0424 - val_loss: 0.0483\n",
      "Epoch 391/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0422 - val_loss: 0.0483\n",
      "Epoch 392/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0424 - val_loss: 0.0553\n",
      "Epoch 393/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 394/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0424 - val_loss: 0.0497\n",
      "Epoch 395/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0424 - val_loss: 0.0507\n",
      "Epoch 396/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0422 - val_loss: 0.0473\n",
      "Epoch 397/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0530\n",
      "Epoch 398/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0452\n",
      "Epoch 399/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0474\n",
      "Epoch 400/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0493\n",
      "Epoch 401/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0422 - val_loss: 0.0550\n",
      "Epoch 402/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0509\n",
      "Epoch 403/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0470\n",
      "Epoch 404/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0497\n",
      "Epoch 405/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0422 - val_loss: 0.0534\n",
      "Epoch 406/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0421 - val_loss: 0.0478\n",
      "Epoch 407/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0422 - val_loss: 0.0474\n",
      "Epoch 408/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0421 - val_loss: 0.0525\n",
      "Epoch 409/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0421 - val_loss: 0.0549\n",
      "Epoch 410/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0421 - val_loss: 0.0457\n",
      "Epoch 411/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0421 - val_loss: 0.0521\n",
      "Epoch 412/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0474\n",
      "Epoch 413/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0555\n",
      "Epoch 414/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0470\n",
      "Epoch 415/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0454\n",
      "Epoch 416/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0419 - val_loss: 0.0502\n",
      "Epoch 417/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0465\n",
      "Epoch 418/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0507\n",
      "Epoch 419/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0492\n",
      "Epoch 420/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0420 - val_loss: 0.0516\n",
      "Epoch 421/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0419 - val_loss: 0.0447\n",
      "Epoch 422/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0419 - val_loss: 0.0543\n",
      "Epoch 423/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0419 - val_loss: 0.0523\n",
      "Epoch 424/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0419 - val_loss: 0.0456\n",
      "Epoch 425/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0419 - val_loss: 0.0427\n",
      "Epoch 426/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0419 - val_loss: 0.0522\n",
      "Epoch 427/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0419 - val_loss: 0.0447\n",
      "Epoch 428/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0420 - val_loss: 0.0452\n",
      "Epoch 429/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0418 - val_loss: 0.0480\n",
      "Epoch 430/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0418 - val_loss: 0.0445\n",
      "Epoch 431/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0418 - val_loss: 0.0493\n",
      "Epoch 432/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0418 - val_loss: 0.0518\n",
      "Epoch 433/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0417 - val_loss: 0.0484\n",
      "Epoch 434/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0417 - val_loss: 0.0493\n",
      "Epoch 435/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0498\n",
      "Epoch 436/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0418 - val_loss: 0.0486\n",
      "Epoch 437/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0418 - val_loss: 0.0492\n",
      "Epoch 438/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0417 - val_loss: 0.0445\n",
      "Epoch 439/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0417 - val_loss: 0.0522\n",
      "Epoch 440/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0418 - val_loss: 0.0511\n",
      "Epoch 441/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0500\n",
      "Epoch 442/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0440\n",
      "Epoch 443/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0416 - val_loss: 0.0471\n",
      "Epoch 444/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0416 - val_loss: 0.0480\n",
      "Epoch 445/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0455\n",
      "Epoch 446/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0458\n",
      "Epoch 447/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0417 - val_loss: 0.0466\n",
      "Epoch 448/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0465\n",
      "Epoch 449/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0415 - val_loss: 0.0463\n",
      "Epoch 450/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0416 - val_loss: 0.0491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x12fc78ae988>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create autoencoder model\n",
    "input_layer = Input(shape=(all_benign.shape[1],))\n",
    "\n",
    "encoded = Dense(110, activation='relu', activity_regularizer=regularizers.l1_l2(l1=10e-6, l2=10e-6))(input_layer)\n",
    "encoded = Dense(95, activation='relu')(encoded)\n",
    "encoded = Dense(20)(encoded)\n",
    "decoded = Dense(95, activation='relu')(encoded)\n",
    "decoded = Dense(110, activation='relu')(decoded)\n",
    "output_layer = Dense(all_benign.shape[1])(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "encoder = Model(input_layer, encoded)\n",
    "autoencoder.compile(optimizer= 'adadelta', loss=rmse)\n",
    "\n",
    "# fit the model\n",
    "autoencoder.fit(all_benign, all_benign, batch_size = 100, epochs = 450, validation_split = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the anomaly data\n",
    "data30 = pd.read_csv(\"3.gafgyt.combo.csv\")\n",
    "data31 = pd.read_csv(\"3.gafgyt.junk.csv\")\n",
    "data32 = pd.read_csv(\"3.gafgyt.scan.csv\")\n",
    "data33 = pd.read_csv(\"3.gafgyt.tcp.csv\")\n",
    "data34 = pd.read_csv(\"3.gafgyt.udp.csv\")\n",
    "\n",
    "data50 = pd.read_csv(\"5.gafgyt.combo.csv\")\n",
    "data51 = pd.read_csv(\"5.gafgyt.junk.csv\")\n",
    "data52 = pd.read_csv(\"5.gafgyt.scan.csv\")\n",
    "data53 = pd.read_csv(\"5.gafgyt.tcp.csv\")\n",
    "data54 = pd.read_csv(\"5.gafgyt.udp.csv\")\n",
    "data55 = pd.read_csv(\"5.mirai.ack.csv\")\n",
    "data56 = pd.read_csv(\"5.mirai.scan.csv\")\n",
    "data57 = pd.read_csv(\"5.mirai.syn.csv\")\n",
    "data58 = pd.read_csv(\"5.mirai.udp.csv\")\n",
    "data59 = pd.read_csv(\"5.mirai.udpplain.csv\")\n",
    "\n",
    "data10 = pd.read_csv(\"1.gafgyt.combo.csv\")\n",
    "data11 = pd.read_csv(\"1.gafgyt.junk.csv\")\n",
    "data12 = pd.read_csv(\"1.gafgyt.scan.csv\")\n",
    "data13 = pd.read_csv(\"1.gafgyt.tcp.csv\")\n",
    "data14 = pd.read_csv(\"1.gafgyt.udp.csv\")\n",
    "data15 = pd.read_csv(\"1.mirai.ack.csv\")\n",
    "data16 = pd.read_csv(\"1.mirai.scan.csv\")\n",
    "data17 = pd.read_csv(\"1.mirai.syn.csv\")\n",
    "data18 = pd.read_csv(\"1.mirai.udp.csv\")\n",
    "data19 = pd.read_csv(\"1.mirai.udpplain.csv\")\n",
    "\n",
    "data60 = pd.read_csv(\"6.gafgyt.combo.csv\")\n",
    "data61 = pd.read_csv(\"6.gafgyt.junk.csv\")\n",
    "data62 = pd.read_csv(\"6.gafgyt.scan.csv\")\n",
    "data63 = pd.read_csv(\"6.gafgyt.tcp.csv\")\n",
    "data64 = pd.read_csv(\"6.gafgyt.udp.csv\")\n",
    "data65 = pd.read_csv(\"6.mirai.ack.csv\")\n",
    "data66 = pd.read_csv(\"6.mirai.scan.csv\")\n",
    "data67 = pd.read_csv(\"6.mirai.syn.csv\")\n",
    "data68 = pd.read_csv(\"6.mirai.udp.csv\")\n",
    "data69 = pd.read_csv(\"6.mirai.udpplain.csv\")\n",
    "\n",
    "\n",
    "anomaly = pd.concat([data50, data51, data52, data53, data54, data55, data56, data57, data58, data59,\n",
    "                     data60, data61, data62, data63, data64, data65, data66, data67, data68, data69,\n",
    "                     data30, data31, data32, data33, data34,\n",
    "                     data10, data11, data12, data13, data14, data15, data16, data17, data18, data19])\n",
    "\n",
    "column_names = [anomaly.columns.values]\n",
    "\n",
    "#scale anomaly data and label as one\n",
    "anomaly = scaler.transform(anomaly)\n",
    "anomaly = pd.DataFrame(anomaly,columns = column_names)\n",
    "anomaly['label'] = 1\n",
    "\n",
    "#scale benign data and give label zero\n",
    "all_benign_test = scaler.transform(all_benign_test)\n",
    "all_benign_test = pd.DataFrame(all_benign_test, columns = column_names)\n",
    "all_benign_test['label'] = 0\n",
    "\n",
    "#concat benign and anomaly data together\n",
    "test_data = pd.concat([anomaly,all_benign_test])\n",
    "\n",
    "#shuffle the data\n",
    "test_data = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "#compressed the data for other AD model\n",
    "compressed_data = encoder.predict(test_data.iloc[:,:-1])\n",
    "\n",
    "#save compressed data as pkl\n",
    "compressed_train_data, compressed_test_data,compressed_train_label,compressed_test_label = train_test_split(compressed_data, \n",
    "                                                   test_data.iloc[:,-1].values, test_size=0.3, random_state=42)\n",
    "\n",
    "compressed_train_data = pd.DataFrame(compressed_train_data)\n",
    "compressed_train_data['label'] = compressed_train_label\n",
    "\n",
    "compressed_test_data = pd.DataFrame(compressed_test_data)\n",
    "compressed_test_data['label'] = compressed_test_label\n",
    "\n",
    "cluster = pd.concat([compressed_test_data,compressed_train_data])\n",
    "cluster.to_csv('cluster_1356.csv')\n",
    "cluster.to_pickle('cluster_1356.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best RMSE threshold to predict anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data shape: (971430, 116)\n",
      "Test data shape: (1942861, 116)\n"
     ]
    }
   ],
   "source": [
    "# use 33% for validation\n",
    "val_data = test_data.iloc[:(test_data.shape[0]//3),:]\n",
    "print(\"Val data shape:\",val_data.shape)\n",
    "test_data = test_data.iloc[(test_data.shape[0]//3):,:]\n",
    "print(\"Test data shape:\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the validation data\n",
    "compressed_val_data = autoencoder.predict(val_data.iloc[:,:-1])\n",
    "rmse_val356 = np_rmse(compressed_val_data, val_data.iloc[:,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RMSE THreshold: 0.18\n",
      "          0         1\n",
      "0  0.987061  0.012939\n",
      "1  0.000017  0.999983\n",
      "\n",
      " RMSE THreshold: 0.2\n",
      "          0         1\n",
      "0  0.990182  0.009818\n",
      "1  0.000026  0.999974\n",
      "\n",
      " RMSE THreshold: 0.22\n",
      "          0         1\n",
      "0  0.992222  0.007778\n",
      "1  0.000040  0.999960\n",
      "\n",
      " RMSE THreshold: 0.25\n",
      "          0         1\n",
      "0  0.993759  0.006241\n",
      "1  0.288528  0.711472\n",
      "\n",
      " RMSE THreshold: 0.28\n",
      "          0         1\n",
      "0  0.994695  0.005305\n",
      "1  0.288540  0.711460\n",
      "\n",
      " RMSE THreshold: 0.3\n",
      "          0         1\n",
      "0  0.995079  0.004921\n",
      "1  0.288548  0.711452\n"
     ]
    }
   ],
   "source": [
    "#Find the best threshold for the rmse using validation data\n",
    "rmse_list = [0.18, 0.2, 0.22, 0.25, 0.28,0.3]\n",
    "\n",
    "for i in rmse_list:\n",
    "    prd_val_356 = predict(rmse_val356,i)\n",
    "    print('\\n RMSE THreshold:',i)\n",
    "    cf = pd.DataFrame(confusion_matrix(val_data.iloc[:,-1].values,prd_val_356,normalize = 'true',labels=[0,1]), \n",
    "                      index=['0', '1'],\n",
    "                     columns = ['0', '1'])\n",
    "    \n",
    "    print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate AE as AD for IoT 1,3,5 and 6 using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict test data\n",
    "compressed_test_data = autoencoder.predict(test_data.iloc[:,:-1])\n",
    "rmse356 = np_rmse(compressed_test_data, test_data.iloc[:,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best threshold from validation step\n",
    "prd_test_356 = predict(rmse356,0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = pd.DataFrame(confusion_matrix(test_data.iloc[:,-1].values, prd_test_356,normalize = 'true',labels=[0,1]), \n",
    "                  index=['0', '1'],\n",
    "                 columns = ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualise confusion matrix\n",
    "import seaborn as sns; sns.set()\n",
    "size = (8, 6)\n",
    "fig, ax = plt.subplots(figsize=size)\n",
    "ax = sns.heatmap(cf.round(2),ax = ax, cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990578</td>\n",
       "      <td>0.009422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.999970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.990578  0.009422\n",
       "1  0.000030  0.999970"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995691920317511"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_data.iloc[:,-1].values, prd_test_356,normalize = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no unseed device for this cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
