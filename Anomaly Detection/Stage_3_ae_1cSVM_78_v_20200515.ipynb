{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: 1cSVM for Anomaly Detection for IoT 7 and 8\n",
    "The 1cSVM  anomaly detection model used in this project work as follows:\n",
    "## 1. We  compressed the data using  AE model. The compressed  data  for devise 7&8 from AE output  was used as input data for 1cSVM.\n",
    "## 2. The data was split into train and test and put through 1cSVM and a confusion matrix was drawn.\n",
    "##  3. After testing  a new device [9] was put though the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mmae.multimodal_autoencoder import MultimodalAutoencoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend, Model, regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49367, 115)\n"
     ]
    }
   ],
   "source": [
    "# Import and stitch all benign data together\n",
    "\n",
    "device7 = pd.read_csv(\"7.benign.csv\")\n",
    "device8 = pd.read_csv(\"8.benign.csv\")\n",
    "\n",
    "\n",
    "all_benign = pd.concat([device7,device8])\n",
    "all_benign['label'] = 0\n",
    "\n",
    "cut_off = int(all_benign.shape[0]/2)\n",
    "\n",
    "# Split the benign data into train and test\n",
    "all_benign, all_benign_test,_,_ = train_test_split(all_benign.iloc[:,:-1].values, \n",
    "                                all_benign.iloc[:,-1].values, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the train data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_benign)\n",
    "all_benign = scaler.transform(all_benign)\n",
    "\n",
    "print(all_benign.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39493 samples, validate on 9874 samples\n",
      "Epoch 1/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.3583 - val_loss: 0.2539\n",
      "Epoch 2/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.2134 - val_loss: 0.1899\n",
      "Epoch 3/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.1705 - val_loss: 0.1503\n",
      "Epoch 4/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.1471 - val_loss: 0.1325\n",
      "Epoch 5/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.1328 - val_loss: 0.1290\n",
      "Epoch 6/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.1235 - val_loss: 0.1096\n",
      "Epoch 7/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.1161 - val_loss: 0.1086\n",
      "Epoch 8/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.1109 - val_loss: 0.1060\n",
      "Epoch 9/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.1061 - val_loss: 0.0988\n",
      "Epoch 10/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.1024 - val_loss: 0.0994\n",
      "Epoch 11/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0995 - val_loss: 0.0935\n",
      "Epoch 12/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0958 - val_loss: 0.1045\n",
      "Epoch 13/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0931 - val_loss: 0.0891\n",
      "Epoch 14/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0904 - val_loss: 0.0900\n",
      "Epoch 15/450\n",
      "39493/39493 [==============================] - 2s 57us/step - loss: 0.0881 - val_loss: 0.0897\n",
      "Epoch 16/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0860 - val_loss: 0.0861\n",
      "Epoch 17/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0841 - val_loss: 0.0792\n",
      "Epoch 18/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0826 - val_loss: 0.0763\n",
      "Epoch 19/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0812 - val_loss: 0.0746\n",
      "Epoch 20/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0797 - val_loss: 0.0760\n",
      "Epoch 21/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0781 - val_loss: 0.0786\n",
      "Epoch 22/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0770 - val_loss: 0.0719\n",
      "Epoch 23/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0761 - val_loss: 0.0768\n",
      "Epoch 24/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0751 - val_loss: 0.0729\n",
      "Epoch 25/450\n",
      "39493/39493 [==============================] - 2s 45us/step - loss: 0.0742 - val_loss: 0.0726\n",
      "Epoch 26/450\n",
      "39493/39493 [==============================] - 2s 53us/step - loss: 0.0729 - val_loss: 0.0725\n",
      "Epoch 27/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0723 - val_loss: 0.0706\n",
      "Epoch 28/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0711 - val_loss: 0.0756\n",
      "Epoch 29/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0706 - val_loss: 0.0750\n",
      "Epoch 30/450\n",
      "39493/39493 [==============================] - 2s 52us/step - loss: 0.0700 - val_loss: 0.0681\n",
      "Epoch 31/450\n",
      "39493/39493 [==============================] - 2s 54us/step - loss: 0.0694 - val_loss: 0.0722\n",
      "Epoch 32/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0686 - val_loss: 0.0650\n",
      "Epoch 33/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0679 - val_loss: 0.0674\n",
      "Epoch 34/450\n",
      "39493/39493 [==============================] - 2s 47us/step - loss: 0.0672 - val_loss: 0.0640\n",
      "Epoch 35/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0667 - val_loss: 0.0694\n",
      "Epoch 36/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0666 - val_loss: 0.0669\n",
      "Epoch 37/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0657 - val_loss: 0.0623\n",
      "Epoch 38/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0649 - val_loss: 0.0666\n",
      "Epoch 39/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0645 - val_loss: 0.0606\n",
      "Epoch 40/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0639 - val_loss: 0.0649\n",
      "Epoch 41/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0640 - val_loss: 0.0646\n",
      "Epoch 42/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0631 - val_loss: 0.0600\n",
      "Epoch 43/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0630 - val_loss: 0.0592\n",
      "Epoch 44/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0625 - val_loss: 0.0692\n",
      "Epoch 45/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0619 - val_loss: 0.0589\n",
      "Epoch 46/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0615 - val_loss: 0.0634\n",
      "Epoch 47/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0613 - val_loss: 0.0617\n",
      "Epoch 48/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0607 - val_loss: 0.0583\n",
      "Epoch 49/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0602 - val_loss: 0.0609\n",
      "Epoch 50/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0602 - val_loss: 0.0588\n",
      "Epoch 51/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0599 - val_loss: 0.0576\n",
      "Epoch 52/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0595 - val_loss: 0.0587\n",
      "Epoch 53/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0593 - val_loss: 0.0600\n",
      "Epoch 54/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0589 - val_loss: 0.0592\n",
      "Epoch 55/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0586 - val_loss: 0.0575\n",
      "Epoch 56/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0584 - val_loss: 0.0608\n",
      "Epoch 57/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0579 - val_loss: 0.0540\n",
      "Epoch 58/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0582 - val_loss: 0.0594\n",
      "Epoch 59/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0576 - val_loss: 0.0667\n",
      "Epoch 60/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0577 - val_loss: 0.0535\n",
      "Epoch 61/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0572 - val_loss: 0.0570\n",
      "Epoch 62/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0569 - val_loss: 0.0571\n",
      "Epoch 63/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0566 - val_loss: 0.0587\n",
      "Epoch 64/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0563 - val_loss: 0.0568\n",
      "Epoch 65/450\n",
      "39493/39493 [==============================] - 2s 53us/step - loss: 0.0564 - val_loss: 0.0557\n",
      "Epoch 66/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0561 - val_loss: 0.0535\n",
      "Epoch 67/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0557 - val_loss: 0.0552\n",
      "Epoch 68/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0556 - val_loss: 0.0607\n",
      "Epoch 69/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0555 - val_loss: 0.0587\n",
      "Epoch 70/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0552 - val_loss: 0.0533\n",
      "Epoch 71/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0553 - val_loss: 0.0550\n",
      "Epoch 72/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0548 - val_loss: 0.0598\n",
      "Epoch 73/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0547 - val_loss: 0.0533\n",
      "Epoch 74/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0545 - val_loss: 0.0586\n",
      "Epoch 75/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0544 - val_loss: 0.0551\n",
      "Epoch 76/450\n",
      "39493/39493 [==============================] - 2s 53us/step - loss: 0.0541 - val_loss: 0.0530\n",
      "Epoch 77/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0539 - val_loss: 0.0538\n",
      "Epoch 78/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0540 - val_loss: 0.0519\n",
      "Epoch 79/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0537 - val_loss: 0.0539\n",
      "Epoch 80/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0537 - val_loss: 0.0530\n",
      "Epoch 81/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0533 - val_loss: 0.0551\n",
      "Epoch 82/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0534 - val_loss: 0.0555\n",
      "Epoch 83/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0531 - val_loss: 0.0517\n",
      "Epoch 84/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0529 - val_loss: 0.0564\n",
      "Epoch 85/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0527 - val_loss: 0.0508\n",
      "Epoch 86/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0527 - val_loss: 0.0506\n",
      "Epoch 87/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0524 - val_loss: 0.0504\n",
      "Epoch 88/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0525 - val_loss: 0.0528\n",
      "Epoch 89/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0523 - val_loss: 0.0596\n",
      "Epoch 90/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0522 - val_loss: 0.0541\n",
      "Epoch 91/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0520 - val_loss: 0.0512\n",
      "Epoch 92/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0519 - val_loss: 0.0555\n",
      "Epoch 93/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0517 - val_loss: 0.0487\n",
      "Epoch 94/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0514 - val_loss: 0.0520\n",
      "Epoch 95/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0517 - val_loss: 0.0483\n",
      "Epoch 96/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0515 - val_loss: 0.0501\n",
      "Epoch 97/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0510 - val_loss: 0.0526\n",
      "Epoch 98/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0513 - val_loss: 0.0548\n",
      "Epoch 99/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0511 - val_loss: 0.0501\n",
      "Epoch 100/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0511 - val_loss: 0.0501\n",
      "Epoch 101/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0506 - val_loss: 0.0540\n",
      "Epoch 102/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0507 - val_loss: 0.0504\n",
      "Epoch 103/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0507 - val_loss: 0.0565\n",
      "Epoch 104/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0506 - val_loss: 0.0524\n",
      "Epoch 105/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0508 - val_loss: 0.0513\n",
      "Epoch 106/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0504 - val_loss: 0.0524\n",
      "Epoch 107/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0502 - val_loss: 0.0501\n",
      "Epoch 108/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0502 - val_loss: 0.0486\n",
      "Epoch 109/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0502 - val_loss: 0.0459\n",
      "Epoch 110/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0504 - val_loss: 0.0499\n",
      "Epoch 111/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0498 - val_loss: 0.0489\n",
      "Epoch 112/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0499 - val_loss: 0.0501\n",
      "Epoch 113/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0498 - val_loss: 0.0504\n",
      "Epoch 114/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0499 - val_loss: 0.0531\n",
      "Epoch 115/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0497 - val_loss: 0.0504\n",
      "Epoch 116/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0497 - val_loss: 0.0469\n",
      "Epoch 117/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0493 - val_loss: 0.0489\n",
      "Epoch 118/450\n",
      "39493/39493 [==============================] - 2s 45us/step - loss: 0.0493 - val_loss: 0.0490\n",
      "Epoch 119/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0490 - val_loss: 0.0489\n",
      "Epoch 120/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0493 - val_loss: 0.0551\n",
      "Epoch 121/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0492 - val_loss: 0.0510\n",
      "Epoch 122/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0488 - val_loss: 0.0487\n",
      "Epoch 123/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0492 - val_loss: 0.0528\n",
      "Epoch 124/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0486 - val_loss: 0.0482\n",
      "Epoch 125/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0485 - val_loss: 0.0468\n",
      "Epoch 126/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0487 - val_loss: 0.0473\n",
      "Epoch 127/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0486 - val_loss: 0.0494\n",
      "Epoch 128/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0483 - val_loss: 0.0475\n",
      "Epoch 129/450\n",
      "39493/39493 [==============================] - 2s 52us/step - loss: 0.0483 - val_loss: 0.0484\n",
      "Epoch 130/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0482 - val_loss: 0.0491\n",
      "Epoch 131/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0482 - val_loss: 0.0467\n",
      "Epoch 132/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0483 - val_loss: 0.0473\n",
      "Epoch 133/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0479 - val_loss: 0.0501\n",
      "Epoch 134/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0481 - val_loss: 0.0482\n",
      "Epoch 135/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0482 - val_loss: 0.0483\n",
      "Epoch 136/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0480 - val_loss: 0.0486\n",
      "Epoch 137/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0481 - val_loss: 0.0485\n",
      "Epoch 138/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0477 - val_loss: 0.0489\n",
      "Epoch 139/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0476 - val_loss: 0.0462\n",
      "Epoch 140/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0475 - val_loss: 0.0455\n",
      "Epoch 141/450\n",
      "39493/39493 [==============================] - 2s 51us/step - loss: 0.0478 - val_loss: 0.0528\n",
      "Epoch 142/450\n",
      "39493/39493 [==============================] - 2s 48us/step - loss: 0.0474 - val_loss: 0.0457\n",
      "Epoch 143/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0476 - val_loss: 0.0536\n",
      "Epoch 144/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0473 - val_loss: 0.0468\n",
      "Epoch 145/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0473 - val_loss: 0.0549\n",
      "Epoch 146/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0472 - val_loss: 0.0454\n",
      "Epoch 147/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0471 - val_loss: 0.0465\n",
      "Epoch 148/450\n",
      "39493/39493 [==============================] - 2s 47us/step - loss: 0.0470 - val_loss: 0.0475\n",
      "Epoch 149/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0471 - val_loss: 0.0450\n",
      "Epoch 150/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0471 - val_loss: 0.0470\n",
      "Epoch 151/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0469 - val_loss: 0.0532\n",
      "Epoch 152/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0473 - val_loss: 0.0438\n",
      "Epoch 153/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0467 - val_loss: 0.0429\n",
      "Epoch 154/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0468 - val_loss: 0.0492\n",
      "Epoch 155/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0465 - val_loss: 0.0473\n",
      "Epoch 156/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0469 - val_loss: 0.0497\n",
      "Epoch 157/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0468 - val_loss: 0.0488\n",
      "Epoch 158/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0466 - val_loss: 0.0457\n",
      "Epoch 159/450\n",
      "39493/39493 [==============================] - 2s 45us/step - loss: 0.0465 - val_loss: 0.0484\n",
      "Epoch 160/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0463 - val_loss: 0.0460\n",
      "Epoch 161/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0464 - val_loss: 0.0459\n",
      "Epoch 162/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0464 - val_loss: 0.0470\n",
      "Epoch 163/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0460 - val_loss: 0.0441\n",
      "Epoch 164/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0462 - val_loss: 0.0464\n",
      "Epoch 165/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0462 - val_loss: 0.0454\n",
      "Epoch 166/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0462 - val_loss: 0.0461\n",
      "Epoch 167/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0460 - val_loss: 0.0479\n",
      "Epoch 168/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0464 - val_loss: 0.0463\n",
      "Epoch 169/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0458 - val_loss: 0.0470\n",
      "Epoch 170/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0461 - val_loss: 0.0466\n",
      "Epoch 171/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0459 - val_loss: 0.0481\n",
      "Epoch 172/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0461 - val_loss: 0.0453\n",
      "Epoch 173/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0459 - val_loss: 0.0450\n",
      "Epoch 174/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0458 - val_loss: 0.0493\n",
      "Epoch 175/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0459 - val_loss: 0.0477\n",
      "Epoch 176/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0458 - val_loss: 0.0452\n",
      "Epoch 177/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0457 - val_loss: 0.0427\n",
      "Epoch 178/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0456 - val_loss: 0.0500\n",
      "Epoch 179/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0456 - val_loss: 0.0421\n",
      "Epoch 180/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0455 - val_loss: 0.0462\n",
      "Epoch 181/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0454 - val_loss: 0.0455\n",
      "Epoch 182/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0452 - val_loss: 0.0465\n",
      "Epoch 183/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0450 - val_loss: 0.0492\n",
      "Epoch 184/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0450 - val_loss: 0.0464\n",
      "Epoch 185/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0454 - val_loss: 0.0442\n",
      "Epoch 186/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0452 - val_loss: 0.0451\n",
      "Epoch 187/450\n",
      "39493/39493 [==============================] - ETA: 0s - loss: 0.044 - 1s 37us/step - loss: 0.0448 - val_loss: 0.0465\n",
      "Epoch 188/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0451 - val_loss: 0.0467\n",
      "Epoch 189/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0452 - val_loss: 0.0470\n",
      "Epoch 190/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0448 - val_loss: 0.0453\n",
      "Epoch 191/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0450 - val_loss: 0.0480\n",
      "Epoch 192/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0450 - val_loss: 0.0459\n",
      "Epoch 193/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0449 - val_loss: 0.0449\n",
      "Epoch 194/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0449 - val_loss: 0.0456\n",
      "Epoch 195/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0447 - val_loss: 0.0469\n",
      "Epoch 196/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0450 - val_loss: 0.0453\n",
      "Epoch 197/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0448 - val_loss: 0.0446\n",
      "Epoch 198/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0447 - val_loss: 0.0467\n",
      "Epoch 199/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0446 - val_loss: 0.0447\n",
      "Epoch 200/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0449 - val_loss: 0.0470\n",
      "Epoch 201/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0448 - val_loss: 0.0461\n",
      "Epoch 202/450\n",
      "39493/39493 [==============================] - 2s 50us/step - loss: 0.0443 - val_loss: 0.0443\n",
      "Epoch 203/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0447 - val_loss: 0.0458\n",
      "Epoch 204/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0444 - val_loss: 0.0477\n",
      "Epoch 205/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0446 - val_loss: 0.0436\n",
      "Epoch 206/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0445 - val_loss: 0.0453\n",
      "Epoch 207/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0444 - val_loss: 0.0459\n",
      "Epoch 208/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0441 - val_loss: 0.0457\n",
      "Epoch 209/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0444 - val_loss: 0.0464\n",
      "Epoch 210/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0441 - val_loss: 0.0434\n",
      "Epoch 211/450\n",
      "39493/39493 [==============================] - 2s 51us/step - loss: 0.0441 - val_loss: 0.0431\n",
      "Epoch 212/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0440 - val_loss: 0.0437\n",
      "Epoch 213/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0439 - val_loss: 0.0477\n",
      "Epoch 214/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0443 - val_loss: 0.0484\n",
      "Epoch 215/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0442 - val_loss: 0.0447\n",
      "Epoch 216/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0440 - val_loss: 0.0438\n",
      "Epoch 217/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0440 - val_loss: 0.0427\n",
      "Epoch 218/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0436 - val_loss: 0.0439\n",
      "Epoch 219/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0437 - val_loss: 0.0425\n",
      "Epoch 220/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0440 - val_loss: 0.0444\n",
      "Epoch 221/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0437 - val_loss: 0.0496\n",
      "Epoch 222/450\n",
      "39493/39493 [==============================] - 2s 56us/step - loss: 0.0440 - val_loss: 0.0436\n",
      "Epoch 223/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0439 - val_loss: 0.0414\n",
      "Epoch 224/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0440 - val_loss: 0.0434\n",
      "Epoch 225/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0435 - val_loss: 0.0451\n",
      "Epoch 226/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0438 - val_loss: 0.0451\n",
      "Epoch 227/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0437 - val_loss: 0.0440\n",
      "Epoch 228/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0436 - val_loss: 0.0407\n",
      "Epoch 229/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0437 - val_loss: 0.0438\n",
      "Epoch 230/450\n",
      "39493/39493 [==============================] - 1s 33us/step - loss: 0.0435 - val_loss: 0.0451\n",
      "Epoch 231/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0436 - val_loss: 0.0441\n",
      "Epoch 232/450\n",
      "39493/39493 [==============================] - 2s 45us/step - loss: 0.0432 - val_loss: 0.0450\n",
      "Epoch 233/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0435 - val_loss: 0.0434\n",
      "Epoch 234/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0436 - val_loss: 0.0437\n",
      "Epoch 235/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0436 - val_loss: 0.0464\n",
      "Epoch 236/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0436 - val_loss: 0.0445\n",
      "Epoch 237/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0432 - val_loss: 0.0441\n",
      "Epoch 238/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0432 - val_loss: 0.0434\n",
      "Epoch 239/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0431 - val_loss: 0.0424\n",
      "Epoch 240/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0431 - val_loss: 0.0425\n",
      "Epoch 241/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0433 - val_loss: 0.0461\n",
      "Epoch 242/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0431 - val_loss: 0.0426\n",
      "Epoch 243/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0429 - val_loss: 0.0460\n",
      "Epoch 244/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0430 - val_loss: 0.0422\n",
      "Epoch 245/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0432 - val_loss: 0.0430\n",
      "Epoch 246/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0431 - val_loss: 0.0432\n",
      "Epoch 247/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0431 - val_loss: 0.0462\n",
      "Epoch 248/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0429 - val_loss: 0.0423\n",
      "Epoch 249/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0431 - val_loss: 0.0420\n",
      "Epoch 250/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0431 - val_loss: 0.0412\n",
      "Epoch 251/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0426 - val_loss: 0.0448\n",
      "Epoch 252/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0429 - val_loss: 0.0423\n",
      "Epoch 253/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0429 - val_loss: 0.0446\n",
      "Epoch 254/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0426 - val_loss: 0.0447\n",
      "Epoch 255/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0429 - val_loss: 0.0414\n",
      "Epoch 256/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0428 - val_loss: 0.0417\n",
      "Epoch 257/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0427 - val_loss: 0.0445\n",
      "Epoch 258/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0426 - val_loss: 0.0432\n",
      "Epoch 259/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 260/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0426 - val_loss: 0.0431\n",
      "Epoch 261/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0428 - val_loss: 0.0416\n",
      "Epoch 262/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0427 - val_loss: 0.0416\n",
      "Epoch 263/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0426 - val_loss: 0.0406\n",
      "Epoch 264/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0425 - val_loss: 0.0434\n",
      "Epoch 265/450\n",
      "39493/39493 [==============================] - 2s 48us/step - loss: 0.0423 - val_loss: 0.0458\n",
      "Epoch 266/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0427 - val_loss: 0.0428\n",
      "Epoch 267/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0425 - val_loss: 0.0425\n",
      "Epoch 268/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0423 - val_loss: 0.0421\n",
      "Epoch 269/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0424 - val_loss: 0.0452\n",
      "Epoch 270/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0424 - val_loss: 0.0430\n",
      "Epoch 271/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0423 - val_loss: 0.0423\n",
      "Epoch 272/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0425 - val_loss: 0.0414\n",
      "Epoch 273/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0424 - val_loss: 0.0446\n",
      "Epoch 274/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0422 - val_loss: 0.0423\n",
      "Epoch 275/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0423 - val_loss: 0.0437\n",
      "Epoch 276/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0423 - val_loss: 0.0455\n",
      "Epoch 277/450\n",
      "39493/39493 [==============================] - 2s 43us/step - loss: 0.0421 - val_loss: 0.0417\n",
      "Epoch 278/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0420 - val_loss: 0.0420\n",
      "Epoch 279/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0422 - val_loss: 0.0425\n",
      "Epoch 280/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0421 - val_loss: 0.0432\n",
      "Epoch 281/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0424 - val_loss: 0.0417\n",
      "Epoch 282/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0421 - val_loss: 0.0441\n",
      "Epoch 283/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0421 - val_loss: 0.0435\n",
      "Epoch 284/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0422 - val_loss: 0.0428\n",
      "Epoch 285/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0421 - val_loss: 0.0418\n",
      "Epoch 286/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0419 - val_loss: 0.0445\n",
      "Epoch 287/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0421 - val_loss: 0.0402\n",
      "Epoch 288/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0422 - val_loss: 0.0425\n",
      "Epoch 289/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0422 - val_loss: 0.0421\n",
      "Epoch 290/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0420 - val_loss: 0.0445\n",
      "Epoch 291/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0419 - val_loss: 0.0439\n",
      "Epoch 292/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0418 - val_loss: 0.0460\n",
      "Epoch 293/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0419 - val_loss: 0.0425\n",
      "Epoch 294/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0420 - val_loss: 0.0440\n",
      "Epoch 295/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0419 - val_loss: 0.0430\n",
      "Epoch 296/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0417 - val_loss: 0.0419\n",
      "Epoch 297/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0416 - val_loss: 0.0407\n",
      "Epoch 298/450\n",
      "39493/39493 [==============================] - 2s 44us/step - loss: 0.0415 - val_loss: 0.0408\n",
      "Epoch 299/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0418 - val_loss: 0.0416\n",
      "Epoch 300/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0418 - val_loss: 0.0399\n",
      "Epoch 301/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0415 - val_loss: 0.0422\n",
      "Epoch 302/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0416 - val_loss: 0.0427\n",
      "Epoch 303/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0415 - val_loss: 0.0431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0413 - val_loss: 0.0471\n",
      "Epoch 305/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0415 - val_loss: 0.0422\n",
      "Epoch 306/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0417 - val_loss: 0.0403\n",
      "Epoch 307/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0415 - val_loss: 0.0466\n",
      "Epoch 308/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0412 - val_loss: 0.0404\n",
      "Epoch 309/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0415 - val_loss: 0.0409\n",
      "Epoch 310/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0412 - val_loss: 0.0393\n",
      "Epoch 311/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0414 - val_loss: 0.0387\n",
      "Epoch 312/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0413 - val_loss: 0.0439\n",
      "Epoch 313/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0412 - val_loss: 0.0431\n",
      "Epoch 314/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0415 - val_loss: 0.0446\n",
      "Epoch 315/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0412 - val_loss: 0.0421\n",
      "Epoch 316/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0414 - val_loss: 0.0463\n",
      "Epoch 317/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0412 - val_loss: 0.0432\n",
      "Epoch 318/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0413 - val_loss: 0.0396\n",
      "Epoch 319/450\n",
      "39493/39493 [==============================] - 2s 50us/step - loss: 0.0412 - val_loss: 0.0419\n",
      "Epoch 320/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0410 - val_loss: 0.0417\n",
      "Epoch 321/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0412 - val_loss: 0.0443\n",
      "Epoch 322/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0414 - val_loss: 0.0432\n",
      "Epoch 323/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0413 - val_loss: 0.0422\n",
      "Epoch 324/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0411 - val_loss: 0.0415\n",
      "Epoch 325/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0410 - val_loss: 0.0440\n",
      "Epoch 326/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0412 - val_loss: 0.0404\n",
      "Epoch 327/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0410 - val_loss: 0.0460\n",
      "Epoch 328/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0409 - val_loss: 0.0426\n",
      "Epoch 329/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0412 - val_loss: 0.0446\n",
      "Epoch 330/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0411 - val_loss: 0.0463\n",
      "Epoch 331/450\n",
      "39493/39493 [==============================] - 2s 48us/step - loss: 0.0409 - val_loss: 0.0410\n",
      "Epoch 332/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0409 - val_loss: 0.0428\n",
      "Epoch 333/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0408 - val_loss: 0.0421\n",
      "Epoch 334/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 335/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0409 - val_loss: 0.0409\n",
      "Epoch 336/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0409 - val_loss: 0.0411\n",
      "Epoch 337/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0411 - val_loss: 0.0399\n",
      "Epoch 338/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0407 - val_loss: 0.0429\n",
      "Epoch 339/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0413 - val_loss: 0.0409\n",
      "Epoch 340/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0408 - val_loss: 0.0420\n",
      "Epoch 341/450\n",
      "39493/39493 [==============================] - 3s 74us/step - loss: 0.0407 - val_loss: 0.0432\n",
      "Epoch 342/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0408 - val_loss: 0.0444\n",
      "Epoch 343/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0407 - val_loss: 0.0405\n",
      "Epoch 344/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0410 - val_loss: 0.0419\n",
      "Epoch 345/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0409 - val_loss: 0.0412\n",
      "Epoch 346/450\n",
      "39493/39493 [==============================] - 2s 47us/step - loss: 0.0407 - val_loss: 0.0403\n",
      "Epoch 347/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0407 - val_loss: 0.0407\n",
      "Epoch 348/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0407 - val_loss: 0.0435\n",
      "Epoch 349/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0409 - val_loss: 0.0403\n",
      "Epoch 350/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0407 - val_loss: 0.0380\n",
      "Epoch 351/450\n",
      "39493/39493 [==============================] - 2s 49us/step - loss: 0.0406 - val_loss: 0.0423\n",
      "Epoch 352/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0405 - val_loss: 0.0426\n",
      "Epoch 353/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0407 - val_loss: 0.0421\n",
      "Epoch 354/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0407 - val_loss: 0.0409\n",
      "Epoch 355/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0404 - val_loss: 0.0412\n",
      "Epoch 356/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0407 - val_loss: 0.0435\n",
      "Epoch 357/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0407 - val_loss: 0.0393\n",
      "Epoch 358/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0403 - val_loss: 0.0414\n",
      "Epoch 359/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0406 - val_loss: 0.0425\n",
      "Epoch 360/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0406 - val_loss: 0.0431\n",
      "Epoch 361/450\n",
      "39493/39493 [==============================] - 2s 59us/step - loss: 0.0405 - val_loss: 0.0435\n",
      "Epoch 362/450\n",
      "39493/39493 [==============================] - 2s 46us/step - loss: 0.0404 - val_loss: 0.0388\n",
      "Epoch 363/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0404 - val_loss: 0.0417\n",
      "Epoch 364/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0404 - val_loss: 0.0433\n",
      "Epoch 365/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0404 - val_loss: 0.0384\n",
      "Epoch 366/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0405 - val_loss: 0.0435\n",
      "Epoch 367/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0402 - val_loss: 0.0429\n",
      "Epoch 368/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0402 - val_loss: 0.0423\n",
      "Epoch 369/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0404 - val_loss: 0.0400\n",
      "Epoch 370/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0404 - val_loss: 0.0421\n",
      "Epoch 371/450\n",
      "39493/39493 [==============================] - 2s 51us/step - loss: 0.0405 - val_loss: 0.0419\n",
      "Epoch 372/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0403 - val_loss: 0.0410\n",
      "Epoch 373/450\n",
      "39493/39493 [==============================] - 2s 39us/step - loss: 0.0401 - val_loss: 0.0412\n",
      "Epoch 374/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0401 - val_loss: 0.0394\n",
      "Epoch 375/450\n",
      "39493/39493 [==============================] - 1s 38us/step - loss: 0.0405 - val_loss: 0.0418\n",
      "Epoch 376/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0405 - val_loss: 0.0421\n",
      "Epoch 377/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0403 - val_loss: 0.0417\n",
      "Epoch 378/450\n",
      "39493/39493 [==============================] - 2s 38us/step - loss: 0.0404 - val_loss: 0.0408\n",
      "Epoch 379/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0402 - val_loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0402 - val_loss: 0.0426\n",
      "Epoch 381/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0401 - val_loss: 0.0388\n",
      "Epoch 382/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0401 - val_loss: 0.0402\n",
      "Epoch 383/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0402 - val_loss: 0.0391\n",
      "Epoch 384/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0402 - val_loss: 0.0402\n",
      "Epoch 385/450\n",
      "39493/39493 [==============================] - 2s 41us/step - loss: 0.0404 - val_loss: 0.0406\n",
      "Epoch 386/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0402 - val_loss: 0.0403\n",
      "Epoch 387/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0400 - val_loss: 0.0434\n",
      "Epoch 388/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0400 - val_loss: 0.0434\n",
      "Epoch 389/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0401 - val_loss: 0.0400\n",
      "Epoch 390/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0399 - val_loss: 0.0391\n",
      "Epoch 391/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0400 - val_loss: 0.0415\n",
      "Epoch 392/450\n",
      "39493/39493 [==============================] - 2s 40us/step - loss: 0.0399 - val_loss: 0.0450\n",
      "Epoch 393/450\n",
      "39493/39493 [==============================] - 2s 42us/step - loss: 0.0402 - val_loss: 0.0410\n",
      "Epoch 394/450\n",
      "39493/39493 [==============================] - 1s 36us/step - loss: 0.0400 - val_loss: 0.0418\n",
      "Epoch 395/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0402 - val_loss: 0.0402\n",
      "Epoch 396/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0398 - val_loss: 0.0432\n",
      "Epoch 397/450\n",
      "39493/39493 [==============================] - 1s 34us/step - loss: 0.0402 - val_loss: 0.0433\n",
      "Epoch 398/450\n",
      "39493/39493 [==============================] - 1s 37us/step - loss: 0.0399 - val_loss: 0.0400\n",
      "Epoch 399/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0397 - val_loss: 0.0404\n",
      "Epoch 400/450\n",
      "39493/39493 [==============================] - 1s 35us/step - loss: 0.0399 - val_loss: 0.0416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# function to calculate rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Auto Encoder inout layer\n",
    "input_layer = Input(shape=(all_benign.shape[1],))\n",
    "\n",
    "# Encoder part\n",
    "encoded = Dense(110, activation='relu', activity_regularizer=regularizers.l1_l2(l1=10e-6, l2=10e-6))(input_layer)\n",
    "encoded = Dense(95, activation='relu')(encoded)\n",
    "encoded = Dense(20)(encoded)\n",
    "\n",
    "# Decoder part\n",
    "decoded = Dense(95, activation='relu')(encoded)\n",
    "decoded = Dense(110, activation='relu')(decoded)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(all_benign.shape[1])(decoded)\n",
    "\n",
    "# Compile autoencoder\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "encoder = Model(input_layer, encoded)\n",
    "autoencoder.compile(optimizer= 'adadelta', loss=rmse)\n",
    "\n",
    "# fit the model\n",
    "autoencoder.fit(all_benign, all_benign, batch_size = 100, epochs = 450,\n",
    "                validation_split = 0.20, callbacks=[EarlyStopping(patience=50,monitor='val_loss')])\n",
    "\n",
    "# predict on validation and other benign \n",
    "benign1_test = encoder.predict(all_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine anomaly data\n",
    "anomaly = pd.concat([pd.read_csv(\"7.gafgyt.combo.csv\"), pd.read_csv(\"7.gafgyt.junk.csv\"), pd.read_csv(\"7.gafgyt.scan.csv\"),pd.read_csv(\"7.gafgyt.tcp.csv\"), pd.read_csv(\"7.gafgyt.udp.csv\"),pd.read_csv(\"8.gafgyt.combo.csv\"), pd.read_csv(\"8.gafgyt.junk.csv\"), pd.read_csv(\"8.gafgyt.scan.csv\"),pd.read_csv(\"8.gafgyt.tcp.csv\"), pd.read_csv(\"8.gafgyt.udp.csv\"), pd.read_csv(\"8.mirai.ack.csv\"),pd.read_csv(\"8.mirai.scan.csv\"), pd.read_csv(\"8.mirai.syn.csv\"), pd.read_csv(\"8.mirai.udp.csv\"), pd.read_csv(\"8.mirai.udpplain.csv\")])\n",
    "\n",
    "column_names = [anomaly .columns.values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139543, 116)\n",
      "(49368, 116)\n",
      "(49367, 21)\n",
      "            0           1          2           3           4           5  \\\n",
      "0  -64.640724  -75.699188  36.760426   13.909013 -125.218208   38.657703   \n",
      "1   -7.117319   -1.907917  -1.311013   -0.943912    0.062029    2.798764   \n",
      "2 -175.567108 -172.088898   5.381436 -123.033615  -47.375862  215.157684   \n",
      "3   -7.117319   -1.907917  -1.311013   -0.943912    0.062029    2.798764   \n",
      "4 -112.418404 -124.301582   1.696837  -86.563072  -16.310423  147.387589   \n",
      "\n",
      "           6           7           8           9         10          11  \\\n",
      "0  11.824189 -265.506683  203.832382 -120.234169 -90.639069  119.334579   \n",
      "1   4.810143   -0.267949    0.689625   -0.686367   5.473677   -2.049598   \n",
      "2  -1.728666 -207.461792  204.222321  -41.723034  -1.197372   44.859261   \n",
      "3   4.810143   -0.267949    0.689625   -0.686367   5.473677   -2.049598   \n",
      "4  -2.905084 -138.065674  146.273712  -38.438118   3.463380   33.617290   \n",
      "\n",
      "          12         13         14         15          16         17  \\\n",
      "0 -21.979618  90.777580  59.960159  26.768768  -42.328850  16.598249   \n",
      "1   2.648039  -2.512970   0.588426   3.226779    1.129538   0.796916   \n",
      "2 -80.892967 -25.985840 -53.501492  89.223312  181.255997  42.439503   \n",
      "3   2.648039  -2.512970   0.588426   3.226779    1.129538   0.796916   \n",
      "4 -40.051426  -9.409765 -27.079836  45.627052  124.444313  22.568623   \n",
      "\n",
      "           18         19  label  \n",
      "0 -236.631851   9.505519      1  \n",
      "1   -2.856642   0.592352      1  \n",
      "2 -213.426605 -48.761158      1  \n",
      "3   -2.856642   0.592352      1  \n",
      "4 -142.522324 -31.139093      1  \n",
      "              0         1         2         3         4         5         6  \\\n",
      "49362  0.086370  0.070533  0.095880  0.060414  0.062739 -0.167521 -0.238170   \n",
      "49363  0.099701 -0.295114  0.021309  0.048986  0.000953 -0.102505  0.045944   \n",
      "49364  0.322331 -0.324100 -0.174869  0.022631 -0.109762  0.005870  0.033419   \n",
      "49365  0.006411  0.156549 -0.009474  0.081854 -0.033480 -0.003933  0.101488   \n",
      "49366  0.350569  0.048690 -1.150215  1.269211  2.049480  1.508622  0.233567   \n",
      "\n",
      "              7         8         9        10        11        12        13  \\\n",
      "49362 -0.480410 -0.204864 -0.193561  0.072018 -0.034093  0.275537  0.239882   \n",
      "49363  0.167826  0.074181 -0.270657  0.033871 -0.100413 -0.140644  0.037287   \n",
      "49364  0.051175  0.041836 -0.314941 -0.096145  0.028309 -0.111970 -0.212005   \n",
      "49365  0.011741 -0.309991  0.139670 -0.056426  0.200091  0.180333  0.071797   \n",
      "49366 -1.857799 -0.912496  0.243911  0.286607 -0.533467  2.158587 -1.229216   \n",
      "\n",
      "             14        15        16        17        18        19  label  \n",
      "49362  0.262408  0.131615  0.116761 -0.189698  0.054652  0.206351      0  \n",
      "49363 -0.048907 -0.017235 -0.181993 -0.025506  0.131773 -0.007944      0  \n",
      "49364 -0.285986 -0.139972  0.166114  0.306524  0.138624  0.070152      0  \n",
      "49365  0.383525 -0.063218  0.111589  0.051084 -0.032076 -0.065665      0  \n",
      "49366 -1.614293  4.052836 -0.867473  0.525968  1.179807  0.180103      0  \n",
      "(1238278, 21)\n"
     ]
    }
   ],
   "source": [
    "# Scale and label anomaly data\n",
    "anomaly = scaler.transform(anomaly)\n",
    "anomaly = pd.DataFrame(anomaly,columns = column_names)\n",
    "anomaly['label'] = 1\n",
    "\n",
    "# Scale and label benign test data\n",
    "all_benign_test = scaler.transform(all_benign_test)\n",
    "all_benign_test = pd.DataFrame(all_benign_test, columns = column_names)\n",
    "all_benign_test['label'] = 0\n",
    "\n",
    "#get the intial training data as well\n",
    "benign1_test = pd.DataFrame(benign1_test)\n",
    "benign1_test['label'] = 0\n",
    "\n",
    "print(anomaly.shape)\n",
    "print(all_benign_test.shape)\n",
    "print(benign1_test.shape)\n",
    "\n",
    "# Combine benign and anomaly data\n",
    "test_data = pd.concat([anomaly,all_benign_test])\n",
    "\n",
    "test_data = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "compressed_data = encoder.predict(test_data.iloc[:,:-1])\n",
    "compressed_data = pd.DataFrame(compressed_data)\n",
    "compressed_data['label'] = test_data.iloc[:,-1].values\n",
    "compressed_data = pd.concat([compressed_data,benign1_test])\n",
    "\n",
    "del all_benign_test, anomaly, benign1_test, test_data\n",
    "\n",
    "print(compressed_data.head())\n",
    "print(compressed_data.tail())\n",
    "print(compressed_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.073150</td>\n",
       "      <td>-0.148070</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>-0.177787</td>\n",
       "      <td>-0.211695</td>\n",
       "      <td>-0.035594</td>\n",
       "      <td>0.184644</td>\n",
       "      <td>0.199323</td>\n",
       "      <td>0.170821</td>\n",
       "      <td>-0.140831</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.052033</td>\n",
       "      <td>-0.226290</td>\n",
       "      <td>-0.051841</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.206028</td>\n",
       "      <td>0.131335</td>\n",
       "      <td>-0.136266</td>\n",
       "      <td>0.214769</td>\n",
       "      <td>-0.066641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.201085</td>\n",
       "      <td>0.210977</td>\n",
       "      <td>0.250692</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>-0.119585</td>\n",
       "      <td>-0.468449</td>\n",
       "      <td>0.199692</td>\n",
       "      <td>0.129238</td>\n",
       "      <td>0.108742</td>\n",
       "      <td>0.331348</td>\n",
       "      <td>-0.184035</td>\n",
       "      <td>0.109604</td>\n",
       "      <td>0.367551</td>\n",
       "      <td>0.207928</td>\n",
       "      <td>-0.165404</td>\n",
       "      <td>-0.579933</td>\n",
       "      <td>-0.146664</td>\n",
       "      <td>0.289422</td>\n",
       "      <td>-0.141094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-2.869385</td>\n",
       "      <td>0.499699</td>\n",
       "      <td>3.363173</td>\n",
       "      <td>1.307569</td>\n",
       "      <td>-1.766352</td>\n",
       "      <td>0.990319</td>\n",
       "      <td>-0.756660</td>\n",
       "      <td>-0.285249</td>\n",
       "      <td>-1.147055</td>\n",
       "      <td>1.716674</td>\n",
       "      <td>-1.675314</td>\n",
       "      <td>1.516451</td>\n",
       "      <td>0.503657</td>\n",
       "      <td>-1.301855</td>\n",
       "      <td>-6.930199</td>\n",
       "      <td>-0.059917</td>\n",
       "      <td>-1.691387</td>\n",
       "      <td>-1.194737</td>\n",
       "      <td>2.384669</td>\n",
       "      <td>1.084768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.047595</td>\n",
       "      <td>0.271234</td>\n",
       "      <td>-0.361656</td>\n",
       "      <td>0.152302</td>\n",
       "      <td>0.155801</td>\n",
       "      <td>-0.378130</td>\n",
       "      <td>-0.900460</td>\n",
       "      <td>-0.192904</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>-0.604379</td>\n",
       "      <td>0.299349</td>\n",
       "      <td>0.075340</td>\n",
       "      <td>0.233428</td>\n",
       "      <td>-0.140827</td>\n",
       "      <td>-0.575089</td>\n",
       "      <td>-0.348442</td>\n",
       "      <td>0.061787</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.042211</td>\n",
       "      <td>0.225937</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.159740</td>\n",
       "      <td>-0.228109</td>\n",
       "      <td>-0.038167</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>-0.110239</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.046120</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>0.127427</td>\n",
       "      <td>-0.141997</td>\n",
       "      <td>0.095362</td>\n",
       "      <td>-0.050423</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.140065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "20  -0.073150 -0.148070 -0.006054 -0.177787 -0.211695 -0.035594  0.184644   \n",
       "89  -0.201085  0.210977  0.250692 -0.187757  0.033367 -0.119585 -0.468449   \n",
       "132 -2.869385  0.499699  3.363173  1.307569 -1.766352  0.990319 -0.756660   \n",
       "150 -0.047595  0.271234 -0.361656  0.152302  0.155801 -0.378130 -0.900460   \n",
       "169 -0.042211  0.225937  0.119994  0.054492  0.120763  0.003177  0.159740   \n",
       "\n",
       "            7         8         9        10        11        12        13  \\\n",
       "20   0.199323  0.170821 -0.140831 -0.004633 -0.052033 -0.226290 -0.051841   \n",
       "89   0.199692  0.129238  0.108742  0.331348 -0.184035  0.109604  0.367551   \n",
       "132 -0.285249 -1.147055  1.716674 -1.675314  1.516451  0.503657 -1.301855   \n",
       "150 -0.192904  0.011110  0.173823  0.497745 -0.604379  0.299349  0.075340   \n",
       "169 -0.228109 -0.038167 -0.084716 -0.110239  0.007555  0.046120  0.102639   \n",
       "\n",
       "           14        15        16        17        18        19  label  \n",
       "20   0.068216  0.206028  0.131335 -0.136266  0.214769 -0.066641      0  \n",
       "89   0.207928 -0.165404 -0.579933 -0.146664  0.289422 -0.141094      0  \n",
       "132 -6.930199 -0.059917 -1.691387 -1.194737  2.384669  1.084768      0  \n",
       "150  0.233428 -0.140827 -0.575089 -0.348442  0.061787  0.210615      0  \n",
       "169  0.127427 -0.141997  0.095362 -0.050423  0.126247  0.140065      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seperate out benign and anomaly data\n",
    "benign_78 = compressed_data.loc[compressed_data['label'] == 0]\n",
    "anomaly_78 = compressed_data.loc[compressed_data['label'] == 1]\n",
    "\n",
    "benign_78.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98735, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_78.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0           1          2           3           4           5  \\\n",
      "0  -64.640724  -75.699188  36.760426   13.909013 -125.218208   38.657703   \n",
      "1   -7.117319   -1.907917  -1.311013   -0.943912    0.062029    2.798764   \n",
      "2 -175.567108 -172.088898   5.381436 -123.033615  -47.375862  215.157684   \n",
      "3   -7.117319   -1.907917  -1.311013   -0.943912    0.062029    2.798764   \n",
      "4 -112.418404 -124.301582   1.696837  -86.563072  -16.310423  147.387589   \n",
      "\n",
      "           6           7           8           9         10          11  \\\n",
      "0  11.824189 -265.506683  203.832382 -120.234169 -90.639069  119.334579   \n",
      "1   4.810143   -0.267949    0.689625   -0.686367   5.473677   -2.049598   \n",
      "2  -1.728666 -207.461792  204.222321  -41.723034  -1.197372   44.859261   \n",
      "3   4.810143   -0.267949    0.689625   -0.686367   5.473677   -2.049598   \n",
      "4  -2.905084 -138.065674  146.273712  -38.438118   3.463380   33.617290   \n",
      "\n",
      "          12         13         14         15          16         17  \\\n",
      "0 -21.979618  90.777580  59.960159  26.768768  -42.328850  16.598249   \n",
      "1   2.648039  -2.512970   0.588426   3.226779    1.129538   0.796916   \n",
      "2 -80.892967 -25.985840 -53.501492  89.223312  181.255997  42.439503   \n",
      "3   2.648039  -2.512970   0.588426   3.226779    1.129538   0.796916   \n",
      "4 -40.051426  -9.409765 -27.079836  45.627052  124.444313  22.568623   \n",
      "\n",
      "           18         19  label  \n",
      "0 -236.631851   9.505519      1  \n",
      "1   -2.856642   0.592352      1  \n",
      "2 -213.426605 -48.761158      1  \n",
      "3   -2.856642   0.592352      1  \n",
      "4 -142.522324 -31.139093      1  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1139543, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(anomaly_78.head())\n",
    "anomaly_78.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.073150</td>\n",
       "      <td>-0.148070</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>-0.177787</td>\n",
       "      <td>-0.211695</td>\n",
       "      <td>-0.035594</td>\n",
       "      <td>0.184644</td>\n",
       "      <td>0.199323</td>\n",
       "      <td>0.170821</td>\n",
       "      <td>-0.140831</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.052033</td>\n",
       "      <td>-0.226290</td>\n",
       "      <td>-0.051841</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.206028</td>\n",
       "      <td>0.131335</td>\n",
       "      <td>-0.136266</td>\n",
       "      <td>0.214769</td>\n",
       "      <td>-0.066641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.201085</td>\n",
       "      <td>0.210977</td>\n",
       "      <td>0.250692</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>0.033367</td>\n",
       "      <td>-0.119585</td>\n",
       "      <td>-0.468449</td>\n",
       "      <td>0.199692</td>\n",
       "      <td>0.129238</td>\n",
       "      <td>0.108742</td>\n",
       "      <td>0.331348</td>\n",
       "      <td>-0.184035</td>\n",
       "      <td>0.109604</td>\n",
       "      <td>0.367551</td>\n",
       "      <td>0.207928</td>\n",
       "      <td>-0.165404</td>\n",
       "      <td>-0.579933</td>\n",
       "      <td>-0.146664</td>\n",
       "      <td>0.289422</td>\n",
       "      <td>-0.141094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-2.869385</td>\n",
       "      <td>0.499699</td>\n",
       "      <td>3.363173</td>\n",
       "      <td>1.307569</td>\n",
       "      <td>-1.766352</td>\n",
       "      <td>0.990319</td>\n",
       "      <td>-0.756660</td>\n",
       "      <td>-0.285249</td>\n",
       "      <td>-1.147055</td>\n",
       "      <td>1.716674</td>\n",
       "      <td>-1.675314</td>\n",
       "      <td>1.516451</td>\n",
       "      <td>0.503657</td>\n",
       "      <td>-1.301855</td>\n",
       "      <td>-6.930199</td>\n",
       "      <td>-0.059917</td>\n",
       "      <td>-1.691387</td>\n",
       "      <td>-1.194737</td>\n",
       "      <td>2.384669</td>\n",
       "      <td>1.084768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.047595</td>\n",
       "      <td>0.271234</td>\n",
       "      <td>-0.361656</td>\n",
       "      <td>0.152302</td>\n",
       "      <td>0.155801</td>\n",
       "      <td>-0.378130</td>\n",
       "      <td>-0.900460</td>\n",
       "      <td>-0.192904</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>-0.604379</td>\n",
       "      <td>0.299349</td>\n",
       "      <td>0.075340</td>\n",
       "      <td>0.233428</td>\n",
       "      <td>-0.140827</td>\n",
       "      <td>-0.575089</td>\n",
       "      <td>-0.348442</td>\n",
       "      <td>0.061787</td>\n",
       "      <td>0.210615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.042211</td>\n",
       "      <td>0.225937</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.159740</td>\n",
       "      <td>-0.228109</td>\n",
       "      <td>-0.038167</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>-0.110239</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.046120</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>0.127427</td>\n",
       "      <td>-0.141997</td>\n",
       "      <td>0.095362</td>\n",
       "      <td>-0.050423</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.140065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "20  -0.073150 -0.148070 -0.006054 -0.177787 -0.211695 -0.035594  0.184644   \n",
       "89  -0.201085  0.210977  0.250692 -0.187757  0.033367 -0.119585 -0.468449   \n",
       "132 -2.869385  0.499699  3.363173  1.307569 -1.766352  0.990319 -0.756660   \n",
       "150 -0.047595  0.271234 -0.361656  0.152302  0.155801 -0.378130 -0.900460   \n",
       "169 -0.042211  0.225937  0.119994  0.054492  0.120763  0.003177  0.159740   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "20   0.199323  0.170821 -0.140831 -0.004633 -0.052033 -0.226290 -0.051841   \n",
       "89   0.199692  0.129238  0.108742  0.331348 -0.184035  0.109604  0.367551   \n",
       "132 -0.285249 -1.147055  1.716674 -1.675314  1.516451  0.503657 -1.301855   \n",
       "150 -0.192904  0.011110  0.173823  0.497745 -0.604379  0.299349  0.075340   \n",
       "169 -0.228109 -0.038167 -0.084716 -0.110239  0.007555  0.046120  0.102639   \n",
       "\n",
       "           14        15        16        17        18        19  \n",
       "20   0.068216  0.206028  0.131335 -0.136266  0.214769 -0.066641  \n",
       "89   0.207928 -0.165404 -0.579933 -0.146664  0.289422 -0.141094  \n",
       "132 -6.930199 -0.059917 -1.691387 -1.194737  2.384669  1.084768  \n",
       "150  0.233428 -0.140827 -0.575089 -0.348442  0.061787  0.210615  \n",
       "169  0.127427 -0.141997  0.095362 -0.050423  0.126247  0.140065  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the label from benign data\n",
    "benign_78 = benign_78.drop(['label'], axis=1)\n",
    "benign_78.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on benign data into 50% train and 50% test\n",
    "all_benign_train78, all_benign_test78  = train_test_split(benign_78, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a one class SVM on benign train data\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "oneclass78 = OneClassSVM(kernel='rbf', gamma=.02,nu=0.02).fit(all_benign_train78)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\608777\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# create test data\n",
    "\n",
    "# change the all_benign_test label to 1, create the label dataframe and drop the label from original dataframe\n",
    "all_benign_test78['label'] = 0\n",
    "benign_label78 = all_benign_test78['label'] + 1\n",
    "all_benign_test78 = all_benign_test78.drop(['label'], axis=1)\n",
    "\n",
    "# change the anomlay label to -1, create the label dataframe and drop the label from original dataframe\n",
    "anomaly_label78 = anomaly_78['label']-2\n",
    "anomaly_78 = anomaly_78.drop(['label'], axis=1)\n",
    "\n",
    "# append benign and anonlay data to create overall test data and test label\n",
    "test_data78 = all_benign_test78.append(anomaly_78)\n",
    "test_label78 = benign_label78.append(anomaly_label78)\n",
    "\n",
    "# rune the one class svm prediction on test data\n",
    "prediction78 = oneclass78.predict(test_data78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1139339,     204],\n",
       "       [   1048,   48320]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = confusion_matrix(test_label78 , prediction78)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00   1139543\n",
      "           1       1.00      0.98      0.99     49368\n",
      "\n",
      "    accuracy                           1.00   1188911\n",
      "   macro avg       1.00      0.99      0.99   1188911\n",
      "weighted avg       1.00      1.00      1.00   1188911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_label78 , prediction78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.978772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.999821  0.000179\n",
       "1  0.021228  0.978772"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix - Percentage\n",
    "cm = pd.DataFrame(confusion_matrix(test_label78 , prediction78,normalize = 'true',labels=[-1,1]),index=['0', '1'],columns = ['0', '1'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
