{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: 1cSVM for Anomaly Detection for IoT [1,3,5,6]\n",
    "The 1cSVM  anomaly detection model used in this project work as follow:\n",
    "## 1. We  compressed the data using  AE model. The compressed  data from AE output  from that  was used as input data for 1cSVM.\n",
    "## 2. The data was split into train and test and put through 1cSVM and a confusion matrix was drawn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mmae.multimodal_autoencoder import MultimodalAutoencoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend, Model, regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device 1,3,5,6 input data as pkl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124658, 115)\n"
     ]
    }
   ],
   "source": [
    "# Import and stitch all benign data together\n",
    "\n",
    "device1 = pd.read_csv(\"1.benign.csv\")\n",
    "device3 = pd.read_csv(\"3.benign.csv\")\n",
    "device5 = pd.read_csv(\"5.benign.csv\")\n",
    "device6 = pd.read_csv(\"6.benign.csv\")\n",
    "\n",
    "all_benign = pd.concat([device1,device3,device5,device6])\n",
    "all_benign['label'] = 0\n",
    "\n",
    "cut_off = int(all_benign.shape[0]/2)\n",
    "\n",
    "# Split the benign data into train and test\n",
    "all_benign, all_benign_test,_,_ = train_test_split(all_benign.iloc[:,:-1].values, \n",
    "                                all_benign.iloc[:,-1].values, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the train data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_benign)\n",
    "all_benign = scaler.transform(all_benign)\n",
    "\n",
    "print(all_benign.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99726 samples, validate on 24932 samples\n",
      "Epoch 1/450\n",
      "99726/99726 [==============================] - 5s 47us/step - loss: 0.2810 - val_loss: 0.1960\n",
      "Epoch 2/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.1637 - val_loss: 0.1369\n",
      "Epoch 3/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.1323 - val_loss: 0.1643\n",
      "Epoch 4/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.1193 - val_loss: 0.1228\n",
      "Epoch 5/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.1118 - val_loss: 0.1265\n",
      "Epoch 6/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.1058 - val_loss: 0.1157\n",
      "Epoch 7/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.1014 - val_loss: 0.1042\n",
      "Epoch 8/450\n",
      "99726/99726 [==============================] - 3s 32us/step - loss: 0.0982 - val_loss: 0.1073\n",
      "Epoch 9/450\n",
      "99726/99726 [==============================] - 5s 49us/step - loss: 0.0943 - val_loss: 0.0975\n",
      "Epoch 10/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0917 - val_loss: 0.1191\n",
      "Epoch 11/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0893 - val_loss: 0.0901\n",
      "Epoch 12/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0876 - val_loss: 0.0901\n",
      "Epoch 13/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0856 - val_loss: 0.0853\n",
      "Epoch 14/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0835 - val_loss: 0.0836\n",
      "Epoch 15/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0818 - val_loss: 0.0783\n",
      "Epoch 16/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0799 - val_loss: 0.0805\n",
      "Epoch 17/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0786 - val_loss: 0.0762\n",
      "Epoch 18/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0772 - val_loss: 0.0713\n",
      "Epoch 19/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0760 - val_loss: 0.0800\n",
      "Epoch 20/450\n",
      "99726/99726 [==============================] - 6s 59us/step - loss: 0.0748 - val_loss: 0.0773\n",
      "Epoch 21/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0735 - val_loss: 0.0809\n",
      "Epoch 22/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0727 - val_loss: 0.0738\n",
      "Epoch 23/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0717 - val_loss: 0.0942\n",
      "Epoch 24/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0709 - val_loss: 0.0782\n",
      "Epoch 25/450\n",
      "99726/99726 [==============================] - 6s 59us/step - loss: 0.0703 - val_loss: 0.0712\n",
      "Epoch 26/450\n",
      "99726/99726 [==============================] - 5s 48us/step - loss: 0.0695 - val_loss: 0.0708\n",
      "Epoch 27/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0689 - val_loss: 0.0681\n",
      "Epoch 28/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0681 - val_loss: 0.0709\n",
      "Epoch 29/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0676 - val_loss: 0.0722\n",
      "Epoch 30/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0673 - val_loss: 0.0799\n",
      "Epoch 31/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0666 - val_loss: 0.0690\n",
      "Epoch 32/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0661 - val_loss: 0.0740\n",
      "Epoch 33/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0658 - val_loss: 0.0741\n",
      "Epoch 34/450\n",
      "99726/99726 [==============================] - 5s 52us/step - loss: 0.0652 - val_loss: 0.0707\n",
      "Epoch 35/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0649 - val_loss: 0.0635\n",
      "Epoch 36/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0643 - val_loss: 0.0689\n",
      "Epoch 37/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0641 - val_loss: 0.0664\n",
      "Epoch 38/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0638 - val_loss: 0.0733\n",
      "Epoch 39/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0634 - val_loss: 0.0701\n",
      "Epoch 40/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0629 - val_loss: 0.0646\n",
      "Epoch 41/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0627 - val_loss: 0.0732\n",
      "Epoch 42/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0623 - val_loss: 0.0656\n",
      "Epoch 43/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0621 - val_loss: 0.0736\n",
      "Epoch 44/450\n",
      "99726/99726 [==============================] - 5s 49us/step - loss: 0.0619 - val_loss: 0.0633\n",
      "Epoch 45/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0614 - val_loss: 0.0692\n",
      "Epoch 46/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0611 - val_loss: 0.0698\n",
      "Epoch 47/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0610 - val_loss: 0.0614\n",
      "Epoch 48/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0605 - val_loss: 0.0659\n",
      "Epoch 49/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0604 - val_loss: 0.0629\n",
      "Epoch 50/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0601 - val_loss: 0.0713\n",
      "Epoch 51/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0599 - val_loss: 0.0670\n",
      "Epoch 52/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0597 - val_loss: 0.0618\n",
      "Epoch 53/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0594 - val_loss: 0.0604\n",
      "Epoch 54/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0591 - val_loss: 0.0655\n",
      "Epoch 55/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0591 - val_loss: 0.0633\n",
      "Epoch 56/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0588 - val_loss: 0.0678\n",
      "Epoch 57/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0587 - val_loss: 0.0628\n",
      "Epoch 58/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0586 - val_loss: 0.0649\n",
      "Epoch 59/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0584 - val_loss: 0.0616\n",
      "Epoch 60/450\n",
      "99726/99726 [==============================] - 5s 48us/step - loss: 0.0580 - val_loss: 0.0633\n",
      "Epoch 61/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0581 - val_loss: 0.0680\n",
      "Epoch 62/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0576 - val_loss: 0.0611\n",
      "Epoch 63/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0575 - val_loss: 0.0645\n",
      "Epoch 64/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0574 - val_loss: 0.0601\n",
      "Epoch 65/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0574 - val_loss: 0.0585\n",
      "Epoch 66/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0571 - val_loss: 0.0624\n",
      "Epoch 67/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0568 - val_loss: 0.0692\n",
      "Epoch 68/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0567 - val_loss: 0.0653\n",
      "Epoch 69/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0565 - val_loss: 0.0710\n",
      "Epoch 70/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0564 - val_loss: 0.0620\n",
      "Epoch 71/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0563 - val_loss: 0.0607\n",
      "Epoch 72/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0560 - val_loss: 0.0569\n",
      "Epoch 73/450\n",
      "99726/99726 [==============================] - 4s 45us/step - loss: 0.0557 - val_loss: 0.0635\n",
      "Epoch 74/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0556 - val_loss: 0.0583\n",
      "Epoch 75/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0557 - val_loss: 0.0663\n",
      "Epoch 76/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0554 - val_loss: 0.0645\n",
      "Epoch 77/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0553 - val_loss: 0.0586\n",
      "Epoch 78/450\n",
      "99726/99726 [==============================] - 3s 33us/step - loss: 0.0554 - val_loss: 0.0641\n",
      "Epoch 79/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0551 - val_loss: 0.0586\n",
      "Epoch 80/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0550 - val_loss: 0.0616\n",
      "Epoch 81/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0547 - val_loss: 0.0590\n",
      "Epoch 82/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0547 - val_loss: 0.0642\n",
      "Epoch 83/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0545 - val_loss: 0.0579\n",
      "Epoch 84/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0541 - val_loss: 0.0622\n",
      "Epoch 85/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0541 - val_loss: 0.0567\n",
      "Epoch 86/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0541 - val_loss: 0.0654\n",
      "Epoch 87/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0539 - val_loss: 0.0638\n",
      "Epoch 88/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0538 - val_loss: 0.0553\n",
      "Epoch 89/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0538 - val_loss: 0.0651\n",
      "Epoch 90/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0537 - val_loss: 0.0590\n",
      "Epoch 91/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0535 - val_loss: 0.0589\n",
      "Epoch 92/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0534 - val_loss: 0.0597\n",
      "Epoch 93/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0533 - val_loss: 0.0580\n",
      "Epoch 94/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0531 - val_loss: 0.0587\n",
      "Epoch 95/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0532 - val_loss: 0.0552\n",
      "Epoch 96/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0529 - val_loss: 0.0577\n",
      "Epoch 97/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0529 - val_loss: 0.0560\n",
      "Epoch 98/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0528 - val_loss: 0.0615\n",
      "Epoch 99/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0527 - val_loss: 0.0530\n",
      "Epoch 100/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0527 - val_loss: 0.0567\n",
      "Epoch 101/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0525 - val_loss: 0.0591\n",
      "Epoch 102/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0524 - val_loss: 0.0589\n",
      "Epoch 103/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0522 - val_loss: 0.0603\n",
      "Epoch 104/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0522 - val_loss: 0.0610\n",
      "Epoch 105/450\n",
      "99726/99726 [==============================] - 3s 35us/step - loss: 0.0521 - val_loss: 0.0545\n",
      "Epoch 106/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0519 - val_loss: 0.0554\n",
      "Epoch 107/450\n",
      "99726/99726 [==============================] - 5s 47us/step - loss: 0.0519 - val_loss: 0.0598\n",
      "Epoch 108/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0519 - val_loss: 0.0579\n",
      "Epoch 109/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0517 - val_loss: 0.0590\n",
      "Epoch 110/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0516 - val_loss: 0.0545\n",
      "Epoch 111/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0517 - val_loss: 0.0586\n",
      "Epoch 112/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0515 - val_loss: 0.0572\n",
      "Epoch 113/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0515 - val_loss: 0.0570\n",
      "Epoch 114/450\n",
      "99726/99726 [==============================] - 3s 32us/step - loss: 0.0514 - val_loss: 0.0565\n",
      "Epoch 115/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0512 - val_loss: 0.0574\n",
      "Epoch 116/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0512 - val_loss: 0.0612\n",
      "Epoch 117/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0511 - val_loss: 0.0572\n",
      "Epoch 118/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0510 - val_loss: 0.0543\n",
      "Epoch 119/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0510 - val_loss: 0.0543\n",
      "Epoch 120/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0508 - val_loss: 0.0543\n",
      "Epoch 121/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0508 - val_loss: 0.0572\n",
      "Epoch 122/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0507 - val_loss: 0.0589\n",
      "Epoch 123/450\n",
      "99726/99726 [==============================] - 5s 49us/step - loss: 0.0507 - val_loss: 0.0573\n",
      "Epoch 124/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0506 - val_loss: 0.0547\n",
      "Epoch 125/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0506 - val_loss: 0.0553\n",
      "Epoch 126/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0505 - val_loss: 0.0509\n",
      "Epoch 127/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0503 - val_loss: 0.0559\n",
      "Epoch 128/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0504 - val_loss: 0.0553\n",
      "Epoch 129/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0504 - val_loss: 0.0561\n",
      "Epoch 130/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0501 - val_loss: 0.0558\n",
      "Epoch 131/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0502 - val_loss: 0.0518\n",
      "Epoch 132/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0500 - val_loss: 0.0534\n",
      "Epoch 133/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0502 - val_loss: 0.0564\n",
      "Epoch 134/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0500 - val_loss: 0.0531\n",
      "Epoch 135/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0500 - val_loss: 0.0519\n",
      "Epoch 136/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0499 - val_loss: 0.0498\n",
      "Epoch 137/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0498 - val_loss: 0.0623\n",
      "Epoch 138/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0498 - val_loss: 0.0508\n",
      "Epoch 139/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0497 - val_loss: 0.0511\n",
      "Epoch 140/450\n",
      "99726/99726 [==============================] - 4s 45us/step - loss: 0.0495 - val_loss: 0.0612\n",
      "Epoch 141/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0495 - val_loss: 0.0649\n",
      "Epoch 142/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0494 - val_loss: 0.0647\n",
      "Epoch 143/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0492 - val_loss: 0.0586\n",
      "Epoch 144/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0494 - val_loss: 0.0565\n",
      "Epoch 145/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0493 - val_loss: 0.0546\n",
      "Epoch 146/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0493 - val_loss: 0.0557\n",
      "Epoch 147/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0491 - val_loss: 0.0511\n",
      "Epoch 148/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0491 - val_loss: 0.0527\n",
      "Epoch 149/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0492 - val_loss: 0.0549\n",
      "Epoch 150/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0490 - val_loss: 0.0517\n",
      "Epoch 151/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0489 - val_loss: 0.0536\n",
      "Epoch 152/450\n",
      "99726/99726 [==============================] - 4s 44us/step - loss: 0.0489 - val_loss: 0.0561\n",
      "Epoch 153/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0489 - val_loss: 0.0537\n",
      "Epoch 154/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0487 - val_loss: 0.0511\n",
      "Epoch 155/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0488 - val_loss: 0.0558\n",
      "Epoch 156/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0486 - val_loss: 0.0571\n",
      "Epoch 157/450\n",
      "99726/99726 [==============================] - 3s 34us/step - loss: 0.0486 - val_loss: 0.0597\n",
      "Epoch 158/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0485 - val_loss: 0.0589\n",
      "Epoch 159/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0485 - val_loss: 0.0531\n",
      "Epoch 160/450\n",
      "99726/99726 [==============================] - 4s 36us/step - loss: 0.0486 - val_loss: 0.0551\n",
      "Epoch 161/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0484 - val_loss: 0.0519\n",
      "Epoch 162/450\n",
      "99726/99726 [==============================] - 4s 35us/step - loss: 0.0483 - val_loss: 0.0528\n",
      "Epoch 163/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0482 - val_loss: 0.0573\n",
      "Epoch 164/450\n",
      "99726/99726 [==============================] - 5s 51us/step - loss: 0.0482 - val_loss: 0.0579\n",
      "Epoch 165/450\n",
      "99726/99726 [==============================] - 5s 51us/step - loss: 0.0482 - val_loss: 0.0551\n",
      "Epoch 166/450\n",
      "99726/99726 [==============================] - 5s 47us/step - loss: 0.0482 - val_loss: 0.0605\n",
      "Epoch 167/450\n",
      "99726/99726 [==============================] - 5s 48us/step - loss: 0.0480 - val_loss: 0.0584\n",
      "Epoch 168/450\n",
      "99726/99726 [==============================] - 5s 55us/step - loss: 0.0480 - val_loss: 0.0522\n",
      "Epoch 169/450\n",
      "99726/99726 [==============================] - 5s 51us/step - loss: 0.0479 - val_loss: 0.0541\n",
      "Epoch 170/450\n",
      "99726/99726 [==============================] - 5s 51us/step - loss: 0.0479 - val_loss: 0.0646\n",
      "Epoch 171/450\n",
      "99726/99726 [==============================] - 6s 55us/step - loss: 0.0478 - val_loss: 0.0509\n",
      "Epoch 172/450\n",
      "99726/99726 [==============================] - 6s 56us/step - loss: 0.0478 - val_loss: 0.0544\n",
      "Epoch 173/450\n",
      "99726/99726 [==============================] - 5s 49us/step - loss: 0.0479 - val_loss: 0.0546\n",
      "Epoch 174/450\n",
      "99726/99726 [==============================] - 6s 58us/step - loss: 0.0477 - val_loss: 0.0566\n",
      "Epoch 175/450\n",
      "99726/99726 [==============================] - 4s 42us/step - loss: 0.0478 - val_loss: 0.0523\n",
      "Epoch 176/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0476 - val_loss: 0.0540\n",
      "Epoch 177/450\n",
      "99726/99726 [==============================] - 4s 41us/step - loss: 0.0476 - val_loss: 0.0547\n",
      "Epoch 178/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0476 - val_loss: 0.0534\n",
      "Epoch 179/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0475 - val_loss: 0.0551\n",
      "Epoch 180/450\n",
      "99726/99726 [==============================] - 4s 40us/step - loss: 0.0475 - val_loss: 0.0523\n",
      "Epoch 181/450\n",
      "99726/99726 [==============================] - 6s 55us/step - loss: 0.0474 - val_loss: 0.0515\n",
      "Epoch 182/450\n",
      "99726/99726 [==============================] - 4s 43us/step - loss: 0.0474 - val_loss: 0.0508\n",
      "Epoch 183/450\n",
      "99726/99726 [==============================] - 4s 37us/step - loss: 0.0473 - val_loss: 0.0557\n",
      "Epoch 184/450\n",
      "99726/99726 [==============================] - 4s 38us/step - loss: 0.0474 - val_loss: 0.0513\n",
      "Epoch 185/450\n",
      "99726/99726 [==============================] - 5s 46us/step - loss: 0.0472 - val_loss: 0.0501\n",
      "Epoch 186/450\n",
      "99726/99726 [==============================] - 4s 39us/step - loss: 0.0473 - val_loss: 0.0542\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# function to calculate rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# Auto Encoder inout layer\n",
    "input_layer = Input(shape=(all_benign.shape[1],))\n",
    "\n",
    "# Encoder part\n",
    "encoded = Dense(110, activation='relu', activity_regularizer=regularizers.l1_l2(l1=10e-6, l2=10e-6))(input_layer)\n",
    "encoded = Dense(95, activation='relu')(encoded)\n",
    "encoded = Dense(20)(encoded)\n",
    "\n",
    "# Decoder part\n",
    "decoded = Dense(95, activation='relu')(encoded)\n",
    "decoded = Dense(110, activation='relu')(decoded)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(all_benign.shape[1])(decoded)\n",
    "\n",
    "# Compile autoencoder\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "encoder = Model(input_layer, encoded)\n",
    "autoencoder.compile(optimizer= 'adadelta', loss=rmse)\n",
    "\n",
    "# fit the model\n",
    "autoencoder.fit(all_benign, all_benign, batch_size = 100, epochs = 450,\n",
    "                validation_split = 0.20, callbacks=[EarlyStopping(patience=50,monitor='val_loss')])\n",
    "\n",
    "# predict on validation and other benign \n",
    "benign1_test = encoder.predict(all_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine anomaly data\n",
    "anomaly = pd.concat([pd.read_csv(\"1.gafgyt.combo.csv\"), pd.read_csv(\"1.gafgyt.junk.csv\"), pd.read_csv(\"1.gafgyt.scan.csv\"),pd.read_csv(\"1.gafgyt.tcp.csv\"), pd.read_csv(\"1.gafgyt.udp.csv\"), pd.read_csv(\"1.mirai.ack.csv\"),pd.read_csv(\"1.mirai.scan.csv\"), pd.read_csv(\"1.mirai.syn.csv\"), pd.read_csv(\"1.mirai.udp.csv\"), pd.read_csv(\"1.mirai.udpplain.csv\"),pd.read_csv(\"3.gafgyt.combo.csv\"), pd.read_csv(\"3.gafgyt.junk.csv\"), pd.read_csv(\"3.gafgyt.scan.csv\"),pd.read_csv(\"3.gafgyt.tcp.csv\"), pd.read_csv(\"3.gafgyt.udp.csv\"),pd.read_csv(\"5.gafgyt.combo.csv\"), pd.read_csv(\"5.gafgyt.junk.csv\"), pd.read_csv(\"5.gafgyt.scan.csv\"),pd.read_csv(\"5.gafgyt.tcp.csv\"), pd.read_csv(\"5.gafgyt.udp.csv\"), pd.read_csv(\"5.mirai.ack.csv\"),pd.read_csv(\"5.mirai.scan.csv\"), pd.read_csv(\"5.mirai.syn.csv\"), pd.read_csv(\"5.mirai.udp.csv\"), pd.read_csv(\"5.mirai.udpplain.csv\"),pd.read_csv(\"6.gafgyt.combo.csv\"), pd.read_csv(\"6.gafgyt.junk.csv\"), pd.read_csv(\"6.gafgyt.scan.csv\"),pd.read_csv(\"6.gafgyt.tcp.csv\"), pd.read_csv(\"6.gafgyt.udp.csv\"), pd.read_csv(\"6.mirai.ack.csv\"),pd.read_csv(\"6.mirai.scan.csv\"), pd.read_csv(\"6.mirai.syn.csv\"), pd.read_csv(\"6.mirai.udp.csv\"), pd.read_csv(\"6.mirai.udpplain.csv\")])\n",
    "\n",
    "column_names = [anomaly .columns.values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2789633, 116)\n",
      "(124658, 116)\n",
      "(124658, 21)\n",
      "            0          1           2          3           4           5  \\\n",
      "0 -136.372055 -36.018013  -61.650940 -36.265022   72.786034  -92.512077   \n",
      "1   -4.896306   2.543107   -2.749959  -0.095127   -1.660665    1.964495   \n",
      "2   -4.896306   2.543107   -2.749959  -0.095127   -1.660665    1.964495   \n",
      "3 -329.659851 -64.248611 -154.619202  -5.011531  116.735481 -136.044571   \n",
      "4 -111.940994 -21.782181  -52.776093  -1.488158   39.353992  -46.589722   \n",
      "\n",
      "           6           7          8          9         10         11  \\\n",
      "0  -3.196047  -22.852793 -54.583939 -12.091518  40.412018  18.692213   \n",
      "1  -0.977479    4.931548   0.877410  -2.465756  -1.068041  -0.364596   \n",
      "2  -0.977479    4.931548   0.877410  -2.465756  -1.068041  -0.364596   \n",
      "3 -93.104553 -105.933609 -74.801414 -46.731384  78.231842   8.056463   \n",
      "4 -33.005077  -33.291275 -26.322390 -18.796240  24.386293  -0.903177   \n",
      "\n",
      "          12        13          14          15          16         17  \\\n",
      "0 -41.494720 -0.224665  -77.434151  -58.170357  -94.816254 -38.092014   \n",
      "1  -0.744657  5.137087   -3.334114    4.189617   -2.845005   1.299728   \n",
      "2  -0.744657  5.137087   -3.334114    4.189617   -2.845005   1.299728   \n",
      "3 -54.242569  8.822739 -138.761093 -126.942642 -262.552032 -63.012260   \n",
      "4 -16.830301  4.602347  -49.937519  -39.562275  -89.025612 -19.319983   \n",
      "\n",
      "           18         19  Label  \n",
      "0 -153.990067  44.129696      1  \n",
      "1    2.377492  -0.761957      1  \n",
      "2    2.377492  -0.761957      1  \n",
      "3 -326.893280  82.162796      1  \n",
      "4 -105.419685  29.852438      1  \n",
      "               0         1         2         3         4         5         6  \\\n",
      "124653 -0.258442 -0.349717  0.028171 -0.249947 -0.267018 -0.034890  0.276149   \n",
      "124654 -0.032374 -0.682133  0.700271 -0.689193 -0.119297  0.140157 -0.636873   \n",
      "124655 -0.474607 -0.298501  0.389547  0.136765  1.521646 -0.434897  0.293832   \n",
      "124656 -0.197545 -0.403010 -0.173455 -0.591533  0.066540 -0.032507 -0.166764   \n",
      "124657 -0.153678 -0.496933  0.116824 -0.270145 -0.154074  0.107696  0.229317   \n",
      "\n",
      "               7         8         9        10        11        12        13  \\\n",
      "124653 -0.419542 -0.187052 -0.394347  0.057430 -0.011887 -0.255730 -0.164846   \n",
      "124654 -0.064665  0.092881  0.178032  0.151493 -0.327796 -0.057350 -0.200682   \n",
      "124655  0.007421 -0.016421 -0.558285  0.487867  0.061100 -0.035613  0.815249   \n",
      "124656  0.461090  0.724691 -0.118770  0.239187 -0.068614 -0.257360 -0.934187   \n",
      "124657 -0.251949 -0.174365 -0.482862 -0.065620 -0.023102 -0.159277 -0.124930   \n",
      "\n",
      "              14        15        16        17        18        19  Label  \n",
      "124653 -0.333049  0.106051 -0.301296 -0.163991  0.027761  0.187475      0  \n",
      "124654  0.089707 -0.137352 -0.196579  0.809238  0.377831  0.207077      0  \n",
      "124655  0.299694 -0.768282  0.028866 -0.968203  1.103365  0.385623      0  \n",
      "124656 -0.227998 -0.030966  0.093136  0.557063  0.112954  0.247287      0  \n",
      "124657 -0.410327  0.081299 -0.178164 -0.184373 -0.087283  0.194557      0  \n",
      "(3038949, 21)\n"
     ]
    }
   ],
   "source": [
    "# Scale and label anomaly data\n",
    "anomaly = scaler.transform(anomaly)\n",
    "anomaly = pd.DataFrame(anomaly,columns = column_names)\n",
    "anomaly['label'] = 1\n",
    "\n",
    "# Scale and label benign test data\n",
    "all_benign_test = scaler.transform(all_benign_test)\n",
    "all_benign_test = pd.DataFrame(all_benign_test, columns = column_names)\n",
    "all_benign_test['label'] = 0\n",
    "\n",
    "#get the intial training data as well\n",
    "benign1_test = pd.DataFrame(benign1_test)\n",
    "benign1_test['Label'] = 0\n",
    "\n",
    "print(anomaly.shape)\n",
    "print(all_benign_test.shape)\n",
    "print(benign1_test.shape)\n",
    "\n",
    "# Combine benign and anomaly data\n",
    "test_data = pd.concat([anomaly,all_benign_test])\n",
    "\n",
    "test_data = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "compressed_data = encoder.predict(test_data.iloc[:,:-1])\n",
    "compressed_data = pd.DataFrame(compressed_data)\n",
    "compressed_data['Label'] = test_data.iloc[:,-1].values\n",
    "compressed_data = pd.concat([compressed_data,benign1_test])\n",
    "\n",
    "del all_benign_test, anomaly, benign1_test, test_data\n",
    "\n",
    "print(compressed_data.head())\n",
    "print(compressed_data.tail())\n",
    "print(compressed_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.632992</td>\n",
       "      <td>0.116820</td>\n",
       "      <td>-0.541826</td>\n",
       "      <td>1.783621</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>-0.311032</td>\n",
       "      <td>-0.569160</td>\n",
       "      <td>0.795657</td>\n",
       "      <td>0.354573</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.714400</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.710571</td>\n",
       "      <td>0.454172</td>\n",
       "      <td>1.088213</td>\n",
       "      <td>-0.754556</td>\n",
       "      <td>-0.454888</td>\n",
       "      <td>-0.654312</td>\n",
       "      <td>-1.989696</td>\n",
       "      <td>-0.789233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.171656</td>\n",
       "      <td>-0.248414</td>\n",
       "      <td>-0.048502</td>\n",
       "      <td>0.160808</td>\n",
       "      <td>-0.135926</td>\n",
       "      <td>0.100528</td>\n",
       "      <td>0.449040</td>\n",
       "      <td>0.137520</td>\n",
       "      <td>-0.024721</td>\n",
       "      <td>-0.267727</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>-0.039022</td>\n",
       "      <td>-0.173164</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.257075</td>\n",
       "      <td>0.069791</td>\n",
       "      <td>0.107526</td>\n",
       "      <td>-0.369945</td>\n",
       "      <td>0.250403</td>\n",
       "      <td>-0.183586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.143467</td>\n",
       "      <td>1.054214</td>\n",
       "      <td>0.207745</td>\n",
       "      <td>2.439886</td>\n",
       "      <td>0.254439</td>\n",
       "      <td>0.540787</td>\n",
       "      <td>0.406138</td>\n",
       "      <td>0.198254</td>\n",
       "      <td>-0.300267</td>\n",
       "      <td>0.287549</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>-0.441936</td>\n",
       "      <td>0.221852</td>\n",
       "      <td>0.224520</td>\n",
       "      <td>0.948814</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>-0.113230</td>\n",
       "      <td>-0.037674</td>\n",
       "      <td>-1.600441</td>\n",
       "      <td>-0.237981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.116786</td>\n",
       "      <td>-0.142173</td>\n",
       "      <td>0.138746</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>-0.404291</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>-0.213578</td>\n",
       "      <td>0.037728</td>\n",
       "      <td>-0.315667</td>\n",
       "      <td>0.016362</td>\n",
       "      <td>0.089543</td>\n",
       "      <td>-0.014961</td>\n",
       "      <td>-0.045144</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.123795</td>\n",
       "      <td>-0.064604</td>\n",
       "      <td>-0.164441</td>\n",
       "      <td>0.064711</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.057020</td>\n",
       "      <td>0.753345</td>\n",
       "      <td>-0.201065</td>\n",
       "      <td>2.077771</td>\n",
       "      <td>0.286774</td>\n",
       "      <td>0.189470</td>\n",
       "      <td>-0.263567</td>\n",
       "      <td>1.043588</td>\n",
       "      <td>0.309579</td>\n",
       "      <td>0.324522</td>\n",
       "      <td>0.365582</td>\n",
       "      <td>-0.124891</td>\n",
       "      <td>0.057770</td>\n",
       "      <td>0.640336</td>\n",
       "      <td>1.635486</td>\n",
       "      <td>-0.202857</td>\n",
       "      <td>0.346579</td>\n",
       "      <td>-0.289535</td>\n",
       "      <td>-1.173367</td>\n",
       "      <td>-0.866696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "30  -0.632992  0.116820 -0.541826  1.783621  0.282258 -0.311032 -0.569160   \n",
       "31   0.171656 -0.248414 -0.048502  0.160808 -0.135926  0.100528  0.449040   \n",
       "59  -0.143467  1.054214  0.207745  2.439886  0.254439  0.540787  0.406138   \n",
       "65   0.116786 -0.142173  0.138746  0.072872 -0.404291  0.192383  0.097246   \n",
       "119  0.057020  0.753345 -0.201065  2.077771  0.286774  0.189470 -0.263567   \n",
       "\n",
       "            7         8         9        10        11        12        13  \\\n",
       "30   0.795657  0.354573  0.001910  0.714400  0.043639  0.710571  0.454172   \n",
       "31   0.137520 -0.024721 -0.267727  0.050313 -0.039022 -0.173164  0.296946   \n",
       "59   0.198254 -0.300267  0.287549  0.527818 -0.441936  0.221852  0.224520   \n",
       "65  -0.213578  0.037728 -0.315667  0.016362  0.089543 -0.014961 -0.045144   \n",
       "119  1.043588  0.309579  0.324522  0.365582 -0.124891  0.057770  0.640336   \n",
       "\n",
       "           14        15        16        17        18        19  Label  \n",
       "30   1.088213 -0.754556 -0.454888 -0.654312 -1.989696 -0.789233      0  \n",
       "31   0.257075  0.069791  0.107526 -0.369945  0.250403 -0.183586      0  \n",
       "59   0.948814  0.059361 -0.113230 -0.037674 -1.600441 -0.237981      0  \n",
       "65   0.022388  0.123795 -0.064604 -0.164441  0.064711  0.005760      0  \n",
       "119  1.635486 -0.202857  0.346579 -0.289535 -1.173367 -0.866696      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seperate out benign and anomaly data\n",
    "benign_1356 = compressed_data.loc[compressed_data['Label'] == 0]\n",
    "anomaly_1356 = compressed_data.loc[compressed_data['Label'] == 1]\n",
    "\n",
    "benign_1356.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.632992</td>\n",
       "      <td>0.116820</td>\n",
       "      <td>-0.541826</td>\n",
       "      <td>1.783621</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>-0.311032</td>\n",
       "      <td>-0.569160</td>\n",
       "      <td>0.795657</td>\n",
       "      <td>0.354573</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.714400</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.710571</td>\n",
       "      <td>0.454172</td>\n",
       "      <td>1.088213</td>\n",
       "      <td>-0.754556</td>\n",
       "      <td>-0.454888</td>\n",
       "      <td>-0.654312</td>\n",
       "      <td>-1.989696</td>\n",
       "      <td>-0.789233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.171656</td>\n",
       "      <td>-0.248414</td>\n",
       "      <td>-0.048502</td>\n",
       "      <td>0.160808</td>\n",
       "      <td>-0.135926</td>\n",
       "      <td>0.100528</td>\n",
       "      <td>0.449040</td>\n",
       "      <td>0.137520</td>\n",
       "      <td>-0.024721</td>\n",
       "      <td>-0.267727</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>-0.039022</td>\n",
       "      <td>-0.173164</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.257075</td>\n",
       "      <td>0.069791</td>\n",
       "      <td>0.107526</td>\n",
       "      <td>-0.369945</td>\n",
       "      <td>0.250403</td>\n",
       "      <td>-0.183586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.143467</td>\n",
       "      <td>1.054214</td>\n",
       "      <td>0.207745</td>\n",
       "      <td>2.439886</td>\n",
       "      <td>0.254439</td>\n",
       "      <td>0.540787</td>\n",
       "      <td>0.406138</td>\n",
       "      <td>0.198254</td>\n",
       "      <td>-0.300267</td>\n",
       "      <td>0.287549</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>-0.441936</td>\n",
       "      <td>0.221852</td>\n",
       "      <td>0.224520</td>\n",
       "      <td>0.948814</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>-0.113230</td>\n",
       "      <td>-0.037674</td>\n",
       "      <td>-1.600441</td>\n",
       "      <td>-0.237981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.116786</td>\n",
       "      <td>-0.142173</td>\n",
       "      <td>0.138746</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>-0.404291</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>-0.213578</td>\n",
       "      <td>0.037728</td>\n",
       "      <td>-0.315667</td>\n",
       "      <td>0.016362</td>\n",
       "      <td>0.089543</td>\n",
       "      <td>-0.014961</td>\n",
       "      <td>-0.045144</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.123795</td>\n",
       "      <td>-0.064604</td>\n",
       "      <td>-0.164441</td>\n",
       "      <td>0.064711</td>\n",
       "      <td>0.005760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.057020</td>\n",
       "      <td>0.753345</td>\n",
       "      <td>-0.201065</td>\n",
       "      <td>2.077771</td>\n",
       "      <td>0.286774</td>\n",
       "      <td>0.189470</td>\n",
       "      <td>-0.263567</td>\n",
       "      <td>1.043588</td>\n",
       "      <td>0.309579</td>\n",
       "      <td>0.324522</td>\n",
       "      <td>0.365582</td>\n",
       "      <td>-0.124891</td>\n",
       "      <td>0.057770</td>\n",
       "      <td>0.640336</td>\n",
       "      <td>1.635486</td>\n",
       "      <td>-0.202857</td>\n",
       "      <td>0.346579</td>\n",
       "      <td>-0.289535</td>\n",
       "      <td>-1.173367</td>\n",
       "      <td>-0.866696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "30  -0.632992  0.116820 -0.541826  1.783621  0.282258 -0.311032 -0.569160   \n",
       "31   0.171656 -0.248414 -0.048502  0.160808 -0.135926  0.100528  0.449040   \n",
       "59  -0.143467  1.054214  0.207745  2.439886  0.254439  0.540787  0.406138   \n",
       "65   0.116786 -0.142173  0.138746  0.072872 -0.404291  0.192383  0.097246   \n",
       "119  0.057020  0.753345 -0.201065  2.077771  0.286774  0.189470 -0.263567   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "30   0.795657  0.354573  0.001910  0.714400  0.043639  0.710571  0.454172   \n",
       "31   0.137520 -0.024721 -0.267727  0.050313 -0.039022 -0.173164  0.296946   \n",
       "59   0.198254 -0.300267  0.287549  0.527818 -0.441936  0.221852  0.224520   \n",
       "65  -0.213578  0.037728 -0.315667  0.016362  0.089543 -0.014961 -0.045144   \n",
       "119  1.043588  0.309579  0.324522  0.365582 -0.124891  0.057770  0.640336   \n",
       "\n",
       "           14        15        16        17        18        19  \n",
       "30   1.088213 -0.754556 -0.454888 -0.654312 -1.989696 -0.789233  \n",
       "31   0.257075  0.069791  0.107526 -0.369945  0.250403 -0.183586  \n",
       "59   0.948814  0.059361 -0.113230 -0.037674 -1.600441 -0.237981  \n",
       "65   0.022388  0.123795 -0.064604 -0.164441  0.064711  0.005760  \n",
       "119  1.635486 -0.202857  0.346579 -0.289535 -1.173367 -0.866696  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the label from benign data\n",
    "benign_1356 = benign_1356.drop(['Label'], axis=1)\n",
    "benign_1356.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on benign data into 50% train and 50% test\n",
    "all_benign_train1356, all_benign_test1356  = train_test_split(benign_1356, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a one class SVM on benign train data\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "oneclass1356 = OneClassSVM(kernel='rbf', gamma=.02,nu=0.02).fit(all_benign_train1356)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data\n",
    "\n",
    "# change the all_benign_test label to 1, create the label dataframe and drop the label from original dataframe\n",
    "all_benign_test1356['Label'] = 0\n",
    "benign_label1356 = all_benign_test1356['Label'] + 1\n",
    "all_benign_test1356 = all_benign_test1356.drop(['Label'], axis=1)\n",
    "\n",
    "# change the anomlay label to -1, create the label dataframe and drop the label from original dataframe\n",
    "anomaly_label1356 = anomaly_1356['Label']-2\n",
    "anomaly_1356 = anomaly_1356.drop(['Label'], axis=1)\n",
    "\n",
    "# append benign and anonlay data to create overall test data and test label\n",
    "test_data1356 = all_benign_test1356.append(anomaly_1356)\n",
    "test_label1356 = benign_label1356.append(anomaly_label1356)\n",
    "\n",
    "# rune the one class svm prediction on test data\n",
    "prediction1356 = oneclass1356.predict(test_data1356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2789436,     197],\n",
       "       [   2694,  121964]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = confusion_matrix(test_label1356 , prediction1356)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00   2789633\n",
      "           1       1.00      0.98      0.99    124658\n",
      "\n",
      "    accuracy                           1.00   2914291\n",
      "   macro avg       1.00      0.99      0.99   2914291\n",
      "weighted avg       1.00      1.00      1.00   2914291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_label1356 , prediction1356))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021611</td>\n",
       "      <td>0.978389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.999929  0.000071\n",
       "1  0.021611  0.978389"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix - Percentage\n",
    "cm = pd.DataFrame(confusion_matrix(test_label1356 , prediction1356,normalize = 'true',labels=[-1,1]),index=['0', '1'],columns = ['0', '1'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
